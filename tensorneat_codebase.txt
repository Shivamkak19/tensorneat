
=== ./test/mlp_substrate.py ===
from tensorneat.algorithm.hyperneat.substrate.mlp import MLPSubstrate, analysis_substrate

layers = [3, 4, 2]
coor_range = (-1, 1, -1, 1)
nodes = analysis_substrate(layers, coor_range)
print(nodes)

=== ./test/origin_operations_test.py ===
import jax, jax.numpy as jnp
from tensorneat.genome.operations import (
    DefaultMutation,
    DefaultDistance,
    DefaultCrossover,
)
from tensorneat.genome import (
    DefaultGenome,
    DefaultNode,
    DefaultConn,
    OriginNode,
    OriginConn,
)
from tensorneat.genome.utils import add_node, add_conn

origin_genome = DefaultGenome(
    node_gene=OriginNode(response_init_std=1),
    conn_gene=OriginConn(),
    mutation=DefaultMutation(conn_add=1, node_add=1, conn_delete=0, node_delete=0),
    crossover=DefaultCrossover(),
    distance=DefaultDistance(),
    num_inputs=3,
    num_outputs=1,
    max_nodes=6,
    max_conns=6,
)

default_genome = DefaultGenome(
    node_gene=DefaultNode(response_init_std=1),
    conn_gene=DefaultConn(),
    mutation=DefaultMutation(conn_add=1, node_add=1, conn_delete=0, node_delete=0),
    crossover=DefaultCrossover(),
    distance=DefaultDistance(),
    num_inputs=3,
    num_outputs=1,
    max_nodes=6,
    max_conns=6,
)

state = default_genome.setup()
state = origin_genome.setup(state)

randkey = jax.random.PRNGKey(42)


def mutation_default():
    nodes, conns = default_genome.initialize(state, randkey)
    print("old genome:\n", default_genome.repr(state, nodes, conns))

    nodes, conns = default_genome.execute_mutation(
        state,
        randkey,
        nodes,
        conns,
        new_node_key=jnp.asarray(10),
        new_conn_keys=jnp.array([20, 21, 22]),
    )

    # new_conn_keys is not used in default genome
    print("new genome:\n", default_genome.repr(state, nodes, conns))


def mutation_origin():
    nodes, conns = origin_genome.initialize(state, randkey)
    print(conns)
    print("old genome:\n", origin_genome.repr(state, nodes, conns))

    nodes, conns = origin_genome.execute_mutation(
        state,
        randkey,
        nodes,
        conns,
        new_node_key=jnp.asarray(10),
        new_conn_keys=jnp.array([20, 21, 22]),
    )
    print(conns)
    # new_conn_keys is used in origin genome
    print("new genome:\n", origin_genome.repr(state, nodes, conns))

def distance_default():
    nodes, conns = default_genome.initialize(state, randkey)
    nodes = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=default_genome.node_gene.new_identity_attrs(state)
    )
    conns1 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10]),  # in-idx, out-idx
        custom_attrs=default_genome.conn_gene.new_zero_attrs(state)
    )
    conns2 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10]),  # in-idx, out-idx
        custom_attrs=default_genome.conn_gene.new_random_attrs(state, randkey)
    )
    print("genome1:\n", default_genome.repr(state, nodes, conns1))
    print("genome2:\n", default_genome.repr(state, nodes, conns2))

    distance = default_genome.execute_distance(state, nodes, conns1, nodes, conns2)
    print("distance: ", distance)

def distance_origin_case1():
    """
    distance with different historical marker
    """
    nodes, conns = origin_genome.initialize(state, randkey)
    nodes = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=origin_genome.node_gene.new_identity_attrs(state)
    )
    conns1 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 99]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_zero_attrs(state)
    )
    conns2 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 88]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_random_attrs(state, randkey)
    )
    print("genome1:\n", origin_genome.repr(state, nodes, conns1))
    print("genome2:\n", origin_genome.repr(state, nodes, conns2))

    distance = origin_genome.execute_distance(state, nodes, conns1, nodes, conns2)
    print("distance: ", distance)

def distance_origin_case2():
    """
    distance with same historical marker
    """
    nodes, conns = origin_genome.initialize(state, randkey)
    nodes = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=origin_genome.node_gene.new_identity_attrs(state)
    )
    conns1 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 99]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_zero_attrs(state)
    )
    conns2 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 99]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_random_attrs(state, randkey)
    )
    print("genome1:\n", origin_genome.repr(state, nodes, conns1))
    print("genome2:\n", origin_genome.repr(state, nodes, conns2))

    distance = origin_genome.execute_distance(state, nodes, conns1, nodes, conns2)
    print("distance: ", distance)

def crossover_origin_case1():
    """
    crossover with different historical marker
    """
    nodes, conns = origin_genome.initialize(state, randkey)
    nodes = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=origin_genome.node_gene.new_identity_attrs(state)
    )
    conns1 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 99]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_zero_attrs(state)
    )
    conns2 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 88]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_random_attrs(state, randkey)
    )
    print("genome1:\n", origin_genome.repr(state, nodes, conns1))
    print("genome2:\n", origin_genome.repr(state, nodes, conns2))

    # (0, 10)'s weight must be 0 (disjoint gene, use fitter)
    child_nodes, child_conns = origin_genome.execute_crossover(state, randkey, nodes, conns1, nodes, conns2)
    print("child:\n", origin_genome.repr(state, child_nodes, child_conns))

def crossover_origin_case2():
    """
    crossover with same historical marker
    """
    nodes, conns = origin_genome.initialize(state, randkey)
    nodes = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=origin_genome.node_gene.new_identity_attrs(state)
    )
    conns1 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 99]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_zero_attrs(state)
    )
    conns2 = add_conn(
        conns,
        fix_attrs=jnp.array([0, 10, 99]),  # in-idx, out-idx, historical mark
        custom_attrs=origin_genome.conn_gene.new_random_attrs(state, randkey)
    )
    print("genome1:\n", origin_genome.repr(state, nodes, conns1))
    print("genome2:\n", origin_genome.repr(state, nodes, conns2))

    # (0, 10)'s weight might be random or zero (homologous gene)

    # zero case:
    child_nodes, child_conns = origin_genome.execute_crossover(state, jax.random.key(99), nodes, conns1, nodes, conns2)
    print("child_zero:\n", origin_genome.repr(state, child_nodes, child_conns))

    # random case:
    child_nodes, child_conns = origin_genome.execute_crossover(state, jax.random.key(0), nodes, conns1, nodes, conns2)
    print("child_random:\n", origin_genome.repr(state, child_nodes, child_conns))

def crossover_origin_case3():
    """
    test examine it use random gene rather than attribute exchange
    """
    nodes, conns = origin_genome.initialize(state, randkey)
    nodes1 = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=jnp.array([1, 2, 0, 0])
    )
    nodes2 = add_node(
        nodes, 
        fix_attrs=jnp.asarray([10]),
        custom_attrs=jnp.array([100, 200, 0, 0])
    )

    # [1, 2] case
    child_nodes, child_conns = origin_genome.execute_crossover(state, jax.random.key(99), nodes1, conns, nodes2, conns)
    print("child1:\n", origin_genome.repr(state, child_nodes, child_conns))

    # [100, 200] case
    child_nodes, child_conns = origin_genome.execute_crossover(state, jax.random.key(1), nodes1, conns, nodes2, conns)
    print("child2:\n", origin_genome.repr(state, child_nodes, child_conns))


if __name__ == "__main__":
    # mutation_origin()
    # distance_default()
    # distance_origin_case1()
    # distance_origin_case2()
    # crossover_origin_case1()
    # crossover_origin_case2()
    crossover_origin_case3()

=== ./test/test_record_episode.py ===

=== ./test/test_genome.py ===
import jax
from algorithm.neat import *

genome = DefaultGenome(
    num_inputs=3,
    num_outputs=1,
    max_nodes=5,
    max_conns=10,
)


def test_output_work():
    randkey = jax.random.PRNGKey(0)
    state = genome.setup()
    nodes, conns = genome.initialize(state, randkey)
    transformed = genome.transform(state, nodes, conns)
    inputs = jax.random.normal(randkey, (3,))
    output = genome.forward(state, transformed, inputs)
    print(output)

    batch_inputs = jax.random.normal(randkey, (10, 3))
    batch_output = jax.vmap(genome.forward, in_axes=(None, None, 0))(
        state, transformed, batch_inputs
    )
    print(batch_output)

    assert True

=== ./test/__init__.py ===

=== ./test/crossover_mutation.py ===
import jax, jax.numpy as jnp
from tensorneat.common import ACT
from algorithm.neat import *
import numpy as np


def main():
    algorithm = NEAT(
        species=DefaultSpecies(
            genome=DefaultGenome(
                num_inputs=3,
                num_outputs=1,
                max_nodes=100,
                max_conns=100,
            ),
            pop_size=1000,
            species_size=10,
            compatibility_threshold=3.5,
        ),
        mutation=DefaultMutation(
            conn_add=0.4,
            conn_delete=0,
            node_add=0.9,
            node_delete=0,
        ),
    )

    state = algorithm.setup(jax.random.key(0))
    pop_nodes, pop_conns = algorithm.species.ask(state.species)

    batch_transform = jax.vmap(algorithm.genome.transform)
    batch_forward = jax.vmap(algorithm.forward, in_axes=(None, 0))

    for _ in range(50):
        winner, losser = jax.random.randint(state.randkey, (2, 1000), 0, 1000)
        elite_mask = jnp.zeros((1000,), dtype=jnp.bool_)
        elite_mask = elite_mask.at[:5].set(1)

        state = algorithm.create_next_generation(
            jax.random.key(0), state, winner, losser, elite_mask
        )
        pop_nodes, pop_conns = algorithm.species.ask(state.species)

        transforms = batch_transform(pop_nodes, pop_conns)
        outputs = batch_forward(jnp.array([1, 0, 1]), transforms)

        try:
            assert not jnp.any(jnp.isnan(outputs))
        except:
            print(_)


if __name__ == "__main__":
    main()

=== ./test/nan_fitness.py ===
import jax, jax.numpy as jnp
from tensorneat.common import ACT
from algorithm.neat import *
import numpy as np


def main():
    node_path = "../examples/brax/nan_node.npy"
    conn_path = "../examples/brax/nan_conn.npy"
    nodes = np.load(node_path)
    conns = np.load(conn_path)
    nodes, conns = jax.device_put([nodes, conns])

    genome = DefaultGenome(
        num_inputs=8,
        num_outputs=2,
        max_nodes=20,
        max_conns=20,
        node_gene=DefaultNodeGene(
            activation_options=(ACT.tanh,),
            activation_default=ACT.tanh,
        ),
    )

    transformed = genome.transform(nodes, conns)
    seq, nodes, conns = transformed
    print(seq)

    exit(0)
    # print(*transformed, sep='\n')

    key = jax.random.key(0)
    dummy_input = jnp.zeros((8,))
    output = genome.forward(dummy_input, transformed)
    print(output)


if __name__ == "__main__":
    a = jnp.array([1, 3, 5, 6, 8])
    b = jnp.array([1, 2, 3])
    print(jnp.isin(a, b))
    # main()

=== ./test/test_nan_fitness.py ===
import jax, jax.numpy as jnp
from tensorneat.common import ACT
from algorithm.neat import *
import numpy as np


def main():
    node_path = "../examples/brax/nan_node.npy"
    conn_path = "../examples/brax/nan_conn.npy"
    nodes = np.load(node_path)
    conns = np.load(conn_path)
    nodes, conns = jax.device_put([nodes, conns])

    genome = DefaultGenome(
        num_inputs=8,
        num_outputs=2,
        max_nodes=20,
        max_conns=20,
        node_gene=DefaultNodeGene(
            activation_options=(ACT.tanh,),
            activation_default=ACT.tanh,
        ),
    )

    transformed = genome.transform(nodes, conns)
    print(*transformed, sep="\n")

    key = jax.random.key(0)
    dummy_input = jnp.zeros((8,))
    output = genome.forward(dummy_input, transformed)
    print(output)


if __name__ == "__main__":
    main()

=== ./README.md ===
<h1 align="center">
  <a href="https://github.com/EMI-Group/evox">
  <picture>
    <source media="(prefers-color-scheme: dark)" srcset="./imgs/evox_logo_dark.png">
    <source media="(prefers-color-scheme: light)" srcset="./imgs/evox_logo_light.png">
      <img alt="EvoX Logo" height="50" src="./imgs/evox_logo_light.png">
  </picture>
  </a>
  <br>
</h1>

<p align="center">
🌟 TensorNEAT: JAX-based NEAT Library for GPU Acceleration 🌟
</p>

<p align="center">
  <a href="https://arxiv.org/abs/2404.01817">
    <img src="https://img.shields.io/badge/paper-arxiv-red?style=for-the-badge" alt="TensorNEAT Paper on arXiv">
  </a>
</p>

## TensorNEAT @ GECCO 2024
TensorNEAT has been selected to recieve the **[GECCO 2024 Best Paper Award](https://gecco-2024.sigevo.org/Best-Paper-Awards#GECH_NE_Track)** 🏆

Many thanks to everyone who has been supporting TensorNEAT, and we will remain committed to advancing TensorNEAT for future 'open-endedness'!

## Introduction
TensorNEAT is a JAX-based libaray for NeuroEvolution of Augmenting Topologies (NEAT) algorithms, focused on harnessing GPU acceleration to enhance the efficiency of evolving neural network structures for complex tasks. Its core mechanism involves the tensorization of network topologies, enabling parallel processing and significantly boosting computational speed and scalability by leveraging modern hardware accelerators. TensorNEAT is compatible with the [EvoX](https://github.com/EMI-Group/evox/) framework.

## Key Features
- JAX-based network for neuroevolution:
    - **Batch inference** across networks with different architectures, GPU-accelerated.
    - Evolve networks with **irregular structures** and **fully customize** their behavior.
    - Visualize the network and represent it in **mathematical formulas** or **codes**.

- GPU-accelerated NEAT implementation:
    - Run NEAT and HyperNEAT on GPUs.
    - Achieve **500x** speedup compared to CPU-based NEAT libraries.

- Rich in extended content:
    - Compatible with **EvoX** for multi-device and distributed support.
    - Test neuroevolution algorithms on advanced **RL tasks** (Brax, Gymnax).

## Solving RL Tasks
Using the NEAT algorithm to solve RL tasks. Here are some results:

The following animations show the behaviors in Brax environments:

| <img src="./imgs/halfcheetah_animation_200.gif" alt="halfcheetah" width="200"> | <img src="./imgs/hopper_animation_200.gif" alt="hopper" width="200"> | <img src="./imgs/walker2d_animation_200.gif" alt="walker2d" width="200"> |
|:-----------------------------------------------------------------------------:|:--------------------------------------------------------------------:|:------------------------------------------------------------------------:|
| halfcheetah                                                                   | hopper                                                               | walker2d                                                                 |

The following graphs show the network of the control policy generated by the NEAT algorithm:

| <img src="./imgs/halfcheetah_network.svg" alt="halfcheetah_network" width="200"> | <img src="./imgs/hopper_network.svg" alt="hopper_network" width="200"> | <img src="./imgs/walker2d_network.svg" alt="walker2d_network" width="200"> |
|:-----------------------------------------------------------------------------:|:--------------------------------------------------------------------:|:------------------------------------------------------------------------:|
| halfcheetah                                                                   | hopper                                                               | walker2d                                                                 |

You can use these codes for running an RL task (Brax Hopper) in TensorNEAT:
```python
# Import necessary modules
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.problem.rl import BraxEnv
from tensorneat.common import ACT, AGG

# Define the pipeline
pipeline = Pipeline(
    algorithm=NEAT(
        pop_size=1000,
        species_size=20,
        survival_threshold=0.1,
        compatibility_threshold=1.0,
        genome=DefaultGenome(
            num_inputs=11,
            num_outputs=3,
            init_hidden_layers=(),
            node_gene=BiasNode(
                activation_options=ACT.tanh,
                aggregation_options=AGG.sum,
            ),
            output_transform=ACT.tanh,
        ),
    ),
    problem=BraxEnv(
        env_name="hopper",
        max_step=1000,
    ),
    seed=42,
    generation_limit=100,
    fitness_target=5000,
)

# Initialize state
state = pipeline.setup()

# Run until termination
state, best = pipeline.auto_run(state)
```
More examples of RL tasks in TensorNEAT can be found in `./examples/brax` and `./examples/gymnax`.

## Solving Function Fitting Tasks (Symbolic Regression)
You can define your custom function and use the NEAT algorithm to solve the function fitting task.

1. Import necessary modules:
```python
import jax, jax.numpy as jnp
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.problem.func_fit import CustomFuncFit
from tensorneat.common import ACT, AGG
```

2. Define a custom function to be fit, and then create the function fitting problem:
```python
def pagie_polynomial(inputs):
    x, y = inputs
    res = 1 / (1 + jnp.pow(x, -4)) + 1 / (1 + jnp.pow(y, -4))

    # Important! Returns an array with one item, NOT a scalar
    return jnp.array([res])

custom_problem = CustomFuncFit(
    func=pagie_polynomial,
    low_bounds=[-1, -1],
    upper_bounds=[1, 1],
    method="sample",
    num_samples=100,
)
```

3. Define custom activation function for the NEAT algorithm:
```python
def square(x):
    return x ** 2
ACT.add_func("square", square)
```

4. Define the NEAT algorithm:
```python
algorithm = NEAT(
    pop_size=10000,
    species_size=20,
    survival_threshold=0.01,
    genome=DefaultGenome(
        num_inputs=2,
        num_outputs=1,
        init_hidden_layers=(),
        node_gene=BiasNode(
            # Using (identity, inversion, square) 
            # as possible activation functions
            activation_options=[ACT.identity, ACT.inv, ACT.square],
            # Using (sum, product) as possible aggregation functions
            aggregation_options=[AGG.sum, AGG.product],
        ),
        output_transform=ACT.identity,
    ),
)
```

5. Define the Pipeline and then run it:
```python
pipeline = Pipeline(
    algorithm=algorithm,
    problem=custom_problem,
    generation_limit=50,
    fitness_target=-1e-4,
    seed=42,
)

# Initialize state
state = pipeline.setup()
# Run until termination
state, best = pipeline.auto_run(state)
# Show result
pipeline.show(state, best)
```
More examples of function fitting tasks in TensorNEAT can be found in `./examples/func_fit`.

## Basic API Usage
Start your journey with TensorNEAT in a few simple steps:

1. **Import necessary modules**:
```python
from tensorneat.pipeline import Pipeline
from tensorneat import algorithm, genome, problem, common
```

2. **Configure the NEAT algorithm and define a problem**:
```python
algorithm = algorithm.NEAT(
    pop_size=10000,
    species_size=20,
    survival_threshold=0.01,
    genome=genome.DefaultGenome(
        num_inputs=3,
        num_outputs=1,
        output_transform=common.ACT.sigmoid,
    ),
)
problem = problem.XOR3d()
```

3. **Initialize the pipeline and run**:
```python
pipeline = Pipeline(
    algorithm,
    problem,
    generation_limit=200,
    fitness_target=-1e-6,
    seed=42,
)
state = pipeline.setup()
# run until termination
state, best = pipeline.auto_run(state)
# show results
pipeline.show(state, best)
```
  Obtain result in a few generations:
```
Fitness limit reached!
input: [0. 0. 0.], target: [0.], predict: [0.00037953]
input: [0. 0. 1.], target: [1.], predict: [0.9990619]
input: [0. 1. 0.], target: [1.], predict: [0.9991497]
input: [0. 1. 1.], target: [0.], predict: [0.0004661]
input: [1. 0. 0.], target: [1.], predict: [0.998262]
input: [1. 0. 1.], target: [0.], predict: [0.00077246]
input: [1. 1. 0.], target: [0.], predict: [0.00082464]
input: [1. 1. 1.], target: [1.], predict: [0.99909043]
loss: 8.861396736392635e-07
```
4. **Visualize the best network**:
```python
network = algorithm.genome.network_dict(state, *best)
algorithm.genome.visualize(network, save_path="./imgs/xor_network.svg")
```
<div style="text-align: center;">
    <img src="./imgs/xor_network.svg" alt="Visualization of the policy"  width="300" height="300">
</div>

5. **Transform the network to latex formulas or python codes**:
```python
from tensorneat.common.sympy_tools import to_latex_code, to_python_code

sympy_res = algorithm.genome.sympy_func(
    state, network, sympy_output_transform=ACT.obtain_sympy(ACT.sigmoid)
)
latex_code = to_latex_code(*sympy_res)
print(latex_code)

python_code = to_python_code(*sympy_res)
print(python_code)
```
Latex formulas:
```latex
\begin{align}
h_{0} &= \frac{1}{0.27 e^{4.28 i_{1}} + 1}\newline
h_{1} &= \frac{1}{0.3 e^{- 4.8 h_{0} + 9.22 i_{0} + 8.09 i_{1} - 10.24 i_{2}} + 1}\newline
h_{2} &= \frac{1}{2.83 e^{5.66 h_{1} - 6.08 h_{0} - 3.03 i_{2}} + 1}\newline
o_{0} &= \frac{1}{0.68 e^{- 20.86 h_{2} + 11.12 h_{1} + 14.22 i_{0} - 1.96 i_{2}} + 1}\newline
\end{align}
```
Python codes:
```python
h = np.zeros(3)
o = np.zeros(1)
h[0] = 1/(0.269965*exp(4.279962*i[1]) + 1)
h[1] = 1/(0.300038*exp(-4.802896*h[0] + 9.215506*i[0] + 8.091845*i[1] - 10.241107*i[2]) + 1)
h[2] = 1/(2.825013*exp(5.660946*h[1] - 6.083459*h[0] - 3.033361*i[2]) + 1)
o[0] = 1/(0.679321*exp(-20.860441*h[2] + 11.122242*h[1] + 14.216276*i[0] - 1.961642*i[2]) + 1)
```

## Installation
1. Install the correct version of [JAX](https://github.com/google/jax). We recommend `jax >= 0.4.28`.

For cpu version only, you may use:
```
pip install -U jax
```

For nvidia gpus, you may use:
```
pip install -U "jax[cuda12]"
```
For details of installing jax, please check https://github.com/google/jax.


2. Install `tensorneat` from the GitHub source code:
```
pip install git+https://github.com/EMI-Group/tensorneat.git
```


## Multi-device and Distributed Acceleration
TensorNEAT doesn't natively support multi-device or distributed execution, but these features can be accessed via the EvoX framework. EvoX is a high-performance, distributed, GPU-accelerated framework for Evolutionary Algorithms. For more details, visit: [EvoX GitHub](https://github.com/EMI-Group/evox/).

TensorNEAT includes an EvoX Adaptor, which allows TensorNEAT algorithms to run within the EvoX framework. Additionally, TensorNEAT provides a monitor for use with EvoX.

Here is an example of creating an EvoX algorithm and monitor:
```python
from tensorneat.common.evox_adaptors import EvoXAlgorithmAdaptor, TensorNEATMonitor
from tensorneat.algorithm import NEAT
from tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.common import ACT, AGG

# define algorithm in TensorNEAT
neat_algorithm = NEAT(
    pop_size=1000,
    species_size=20,
    survival_threshold=0.1,
    compatibility_threshold=1.0,
    genome=DefaultGenome(
        max_nodes=50,
        max_conns=200,
        num_inputs=17,
        num_outputs=6,
        node_gene=BiasNode(
            activation_options=ACT.tanh,
            aggregation_options=AGG.sum,
        ),
        output_transform=ACT.tanh,
    ),
)
# use adaptor to create EvoX algorithm
evox_algorithm = EvoXAlgorithmAdaptor(neat_algorithm)
# monitor in Evox
monitor = TensorNEATMonitor(neat_algorithm, is_save=False)
```
Using this code, you can run the NEAT algorithm within EvoX and leverage EvoX's multi-device and distributed capabilities. 

For a complete example, see `./example/with_evox/walker2d_evox.py`, which demonstrates EvoX's multi-device functionality.

## HyperNEAT
TensorNEAT also implements the HyperNEAT algorithm. Here is a sample code to use it:

```python
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.algorithm.hyperneat import HyperNEAT, FullSubstrate
from tensorneat.genome import DefaultGenome
from tensorneat.common import ACT

# Create the substrate for HyperNEAT.
# This substrate is used to solve the XOR3d problem (3 inputs).
# input_coors has 4 coordinates because we need an extra one to represent bias.
substrate = FullSubstrate(
    input_coors=((-1, -1), (-0.33, -1), (0.33, -1), (1, -1)),
    hidden_coors=((-1, 0), (0, 0), (1, 0)),
    output_coors=((0, 1),),
)

# The NEAT algorithm calculates the connection strength in the HyperNEAT substrate.
# It has 4 inputs (in-node and out-node coordinates in substrates) and 1 output (connection strength).
neat = NEAT(
    pop_size=10000,
    species_size=20,
    survival_threshold=0.01,
    genome=DefaultGenome(
        num_inputs=4,  # size of query coordinates from the substrate
        num_outputs=1,  # the connection strength
        init_hidden_layers=(),
        output_transform=ACT.tanh,
    ),
)

# Define the HyperNEAT algorithm.
algorithm = HyperNEAT(
    substrate=substrate,
    neat=neat,
    activation=ACT.tanh,
    activate_time=10,
    output_transform=ACT.sigmoid,
)
```

For a complete example, see `./examples/func_fit/xor_hyperneat.py` and `./examples/gymnax/cartpole_hyperneat.py`.

## Future Work

1. Improve TensorNEAT documentation and tutorials.
2. Implement more NEAT-related algorithms, such as ES-HyperNEAT.
3. Add gradient descent support for networks in NEAT.
4. Further optimize TensorNEAT to increase computation speed and reduce memory usage.

We warmly welcome community developers to contribute to TensorNEAT and look forward to your pull requests!


## Community & Support

- Engage in discussions and share your experiences on [GitHub Issues](https://github.com/EMI-Group/tensorneat/issues).
- Join our QQ group (ID: 297969717).


## Acknowledgements

1. Thanks to Kenneth O. Stanley and Risto Miikkulainen for [the NEAT algorithm](https://ieeexplore.ieee.org/abstract/document/6790655), which has greatly advanced neuroevolution.

2. Thanks to the Google team for [JAX](https://github.com/google/jax), making GPU programming easy and efficient.

3. Thanks to [neat-python](https://github.com/CodeReclaimers/neat-python) and [pureples](https://github.com/ukuleleplayer/pureples) for their clear Python implementations of NEAT and HyperNEAT.

4. Thanks to [Brax](https://github.com/google/brax) and [gymnax](https://github.com/RobertTLange/gymnax) for efficient benchmarking frameworks.

5. Thanks to the [EvoX](https://github.com/EMI-Group/evox). Integrating with EvoX allows TensorNEAT to combine the NEAT algorithm with other evolutionary algorithms, expanding its potential. EvoX also provides multi-device and distributed support for TensorNEAT.


## Citing TensorNEAT

If you use TensorNEAT in your research and want to cite it in your work, please use:
```
@inproceedings{10.1145/3638529.3654210,
  author = {Wang, Lishuang and Zhao, Mengfei and Liu, Enyu and Sun, Kebin and Cheng, Ran},
  title = {Tensorized NeuroEvolution of Augmenting Topologies for GPU Acceleration},
  year = {2024},
  isbn = {9798400704949},
  doi = {10.1145/3638529.3654210},
  booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
  pages = {1156–1164},
  numpages = {9},
  keywords = {neuroevolution, GPU acceleration, algorithm library},
  location = {Melbourne, VIC, Australia},
  series = {GECCO '24}
}


=== ./setup.py ===
from setuptools import setup, find_packages

setup(
    name="tensorneat",
    version="0.1",
    packages=find_packages(),
)

=== ./examples/brax/ant.py ===

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode, DefaultConn,DefaultMutation

from tensorneat.problem.rl import BraxEnv
from tensorneat.common import ACT, AGG
import jax
def random_sample_policy(randkey, obs):
    return jax.random.uniform(randkey, (8,), minval=-1.0, maxval=1.0)
if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=3000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=0.8,
            genome=DefaultGenome(
                max_nodes=100,
                max_conns=1500,
                num_inputs=27,
                num_outputs=8,
                init_hidden_layers=(30,),
                mutation=DefaultMutation(
                    node_delete=0.0,
                ),
                node_gene=BiasNode(
                    bias_init_std=0.1,
                    bias_mutate_power=0.05,
                    bias_mutate_rate=0.01,
                    bias_replace_rate=0.0,
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                ),
                conn_gene=DefaultConn(
                    weight_init_mean=0.0,
                    weight_init_std=0.1,
                    weight_mutate_power=0.05,
                    weight_replace_rate=0.0,
                    weight_mutate_rate=0.001,
                ),
                output_transform=ACT.tanh,
            ),
        ),
        problem=BraxEnv(
            env_name="ant",
            max_step=1000,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=8000,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)
=== ./examples/brax/hopper.py ===
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode

from tensorneat.problem.rl import BraxEnv
from tensorneat.common import ACT, AGG

if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                num_inputs=11,
                num_outputs=3,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                ),
                output_transform=ACT.tanh,
            ),
        ),
        problem=BraxEnv(
            env_name="hopper",
            max_step=1000,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=5000,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/brax/halfcheetah.py ===
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode

from tensorneat.problem.rl import BraxEnv
from tensorneat.common import ACT, AGG

import jax


def random_sample_policy(randkey, obs):
    return jax.random.uniform(randkey, (6,), minval=-1.0, maxval=1.0)


if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                max_nodes=50,
                max_conns=200,
                num_inputs=17,
                num_outputs=6,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=ACT.scaled_tanh,
                    aggregation_options=AGG.sum,
                ),
                output_transform=ACT.tanh,
            ),
        ),
        problem=BraxEnv(
            env_name="halfcheetah",
            max_step=1000,
            obs_normalization=True,
            sample_episodes=1000,
            sample_policy=random_sample_policy,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=8000,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/brax/hopper_origin.py ===
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, OriginNode, OriginConn

from tensorneat.problem.rl import BraxEnv
from tensorneat.common import ACT, AGG

"""
Solving Hopper with OriginGene
See https://github.com/EMI-Group/tensorneat/issues/11
"""

if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                num_inputs=11,
                num_outputs=3,
                init_hidden_layers=(),
                # origin node gene, which use the same crossover behavior to the origin NEAT paper
                node_gene=OriginNode(
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                    response_lower_bound = 1,
                    response_upper_bound= 1,  # fix response to 1
                ),
                # use origin connection, which using historical marker
                conn_gene=OriginConn(),  
                output_transform=ACT.tanh,
            ),
        ),
        problem=BraxEnv(
            env_name="hopper",
            max_step=1000,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=5000,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/brax/walker2d.py ===
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode

from tensorneat.problem.rl import BraxEnv
from tensorneat.common import ACT, AGG

import jax, jax.numpy as jnp


def random_sample_policy(randkey, obs):
    return jax.random.uniform(randkey, (6,), minval=-1.0, maxval=1.0)


if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                max_nodes=50,
                max_conns=200,
                num_inputs=17,
                num_outputs=6,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                ),
                output_transform=ACT.tanh,
            ),
        ),
        problem=BraxEnv(
            env_name="walker2d",
            max_step=1000,
            obs_normalization=True,
            sample_episodes=1000,
            sample_policy=random_sample_policy,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=5000,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/func_fit/xor_hyperneat.py ===
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.algorithm.hyperneat import HyperNEAT, FullSubstrate
from tensorneat.genome import DefaultGenome
from tensorneat.common import ACT

from tensorneat.problem.func_fit import XOR3d

if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=HyperNEAT(
            substrate=FullSubstrate(
                input_coors=((-1, -1), (-0.33, -1), (0.33, -1), (1, -1)),
                hidden_coors=((-1, 0), (0, 0), (1, 0)),
                output_coors=((0, 1),),
            ),
            neat=NEAT(
                pop_size=10000,
                species_size=20,
                survival_threshold=0.01,
                genome=DefaultGenome(
                    num_inputs=4,  # size of query coors
                    num_outputs=1,
                    init_hidden_layers=(),
                    output_transform=ACT.tanh,
                ),
            ),
            activation=ACT.tanh,
            activate_time=10,
            output_transform=ACT.sigmoid,
        ),
        problem=XOR3d(),
        generation_limit=300,
        fitness_target=-1e-6,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)
    # show result
    pipeline.show(state, best)

=== ./examples/func_fit/xor_recurrent.py ===
from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import RecurrentGenome
from tensorneat.problem.func_fit import XOR3d
from tensorneat.common import ACT, AGG

if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=10000,
            species_size=20,
            survival_threshold=0.01,
            genome=RecurrentGenome(
                num_inputs=3,
                num_outputs=1,
                init_hidden_layers=(),
                output_transform=ACT.sigmoid,
                activate_time=10,
            ),
        ),
        problem=XOR3d(),
        generation_limit=500,
        fitness_target=-1e-6,  # float32 precision
        seed=42,
    )

    # initialize state
    state = pipeline.setup()
    # run until terminate
    state, best = pipeline.auto_run(state)
    # show result
    pipeline.show(state, best)

=== ./examples/func_fit/xor_origin.py ===
from tensorneat.pipeline import Pipeline
from tensorneat import algorithm, genome, problem
from tensorneat.genome import OriginNode, OriginConn
from tensorneat.common import ACT

"""
Solving XOR-3d problem with OriginGene
See https://github.com/EMI-Group/tensorneat/issues/11
"""

algorithm = algorithm.NEAT(
    pop_size=10000,
    species_size=20,
    survival_threshold=0.01,
    genome=genome.DefaultGenome(
        node_gene=OriginNode(),
        conn_gene=OriginConn(),
        num_inputs=3,
        num_outputs=1,
        max_nodes=7,
        output_transform=ACT.sigmoid,
    ),
)
problem = problem.XOR3d()

pipeline = Pipeline(
    algorithm,
    problem,
    generation_limit=200,
    fitness_target=-1e-6,
    seed=42,
)
state = pipeline.setup()
# run until terminate
state, best = pipeline.auto_run(state)
# show result
pipeline.show(state, best)

# visualize the best individual
network = algorithm.genome.network_dict(state, *best)
print(algorithm.genome.repr(state, *best))
# algorithm.genome.visualize(network, save_path="./imgs/xor_network.svg")

# transform the best individual to latex formula
from tensorneat.common.sympy_tools import to_latex_code, to_python_code

sympy_res = algorithm.genome.sympy_func(
    state, network, sympy_output_transform=ACT.obtain_sympy(ACT.sigmoid)
)
latex_code = to_latex_code(*sympy_res)
print(latex_code)

# transform the best individual to python code
python_code = to_python_code(*sympy_res)
print(python_code)

=== ./examples/func_fit/xor_hyperneat_feedforward.py ===
import jax.numpy as jnp

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.algorithm.hyperneat import HyperNEATFeedForward, MLPSubstrate
from tensorneat.genome import DefaultGenome
from tensorneat.common import ACT

from tensorneat.problem.func_fit import XOR3d

if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=HyperNEATFeedForward(
            substrate=MLPSubstrate(
                layers=[4, 5, 5, 5, 1], coor_range=(-5.0, 5.0, -5.0, 5.0)
            ),
            neat=NEAT(
                pop_size=10000,
                species_size=20,
                survival_threshold=0.01,
                genome=DefaultGenome(
                    num_inputs=4,  # size of query coors
                    num_outputs=1,
                    init_hidden_layers=(),
                    output_transform=ACT.tanh,
                ),
            ),
            activation=ACT.tanh,
            output_transform=ACT.sigmoid,
        ),
        problem=XOR3d(),
        generation_limit=1000,
        fitness_target=-1e-5,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)
    # show result
    pipeline.show(state, best)

    # visualize cppn
    cppn_genome = pipeline.algorithm.neat.genome
    cppn_network = cppn_genome.network_dict(state, *best)
    cppn_genome.visualize(cppn_network, save_path="./imgs/cppn_network.svg")

    # visualize hyperneat genome
    hyperneat_genome = pipeline.algorithm.hyper_genome
    # use cppn to calculate the weights in hyperneat genome
    # return seqs, nodes, conns, u_conns
    _, hyperneat_nodes, hyperneat_conns, _ = pipeline.algorithm.transform(state, best)
    # mutate the connection with weight 0 (to visualize the network rather the substrate)
    hyperneat_conns = jnp.where(
        hyperneat_conns[:, 2][:, None] == 0, jnp.nan, hyperneat_conns
    )
    hyperneat_network = hyperneat_genome.network_dict(
        state, hyperneat_nodes, hyperneat_conns
    )
    hyperneat_genome.visualize(
        hyperneat_network, save_path="./imgs/hyperneat_network.svg"
    )

=== ./examples/func_fit/xor.py ===
from tensorneat.pipeline import Pipeline
from tensorneat import algorithm, genome, problem
from tensorneat.common import ACT

algorithm = algorithm.NEAT(
    pop_size=10000,
    species_size=20,
    survival_threshold=0.01,
    genome=genome.DefaultGenome(
        num_inputs=3,
        num_outputs=1,
        max_nodes=7,
        output_transform=ACT.sigmoid,
    ),
)
problem = problem.XOR3d()

pipeline = Pipeline(
    algorithm,
    problem,
    generation_limit=200,
    fitness_target=-1e-6,
    seed=42,
)
state = pipeline.setup()
# run until terminate
state, best = pipeline.auto_run(state)
# show result
pipeline.show(state, best)

# visualize the best individual
network = algorithm.genome.network_dict(state, *best)
print(algorithm.genome.repr(state, *best))
# algorithm.genome.visualize(network, save_path="./imgs/xor_network.svg")

# transform the best individual to latex formula
from tensorneat.common.sympy_tools import to_latex_code, to_python_code

sympy_res = algorithm.genome.sympy_func(
    state, network, sympy_output_transform=ACT.obtain_sympy(ACT.sigmoid)
)
latex_code = to_latex_code(*sympy_res)
print(latex_code)

# transform the best individual to python code
python_code = to_python_code(*sympy_res)
print(python_code)

=== ./examples/func_fit/custom_func_fit.py ===
import jax.numpy as jnp

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.problem.func_fit import CustomFuncFit
from tensorneat.common import ACT, AGG

# define a custom function fit problem
def pagie_polynomial(inputs):
    x, y = inputs
    res = 1 / (1 + jnp.pow(x, -4)) + 1 / (1 + jnp.pow(y, -4))

    # important! returns an array, NOT a scalar
    return jnp.array([res])

# define custom activate function and register it
def square(x):
    return x ** 2
ACT.add_func("square", square)

if __name__ == "__main__":
    custom_problem = CustomFuncFit(
        func=pagie_polynomial,
        low_bounds=[-1, -1],
        upper_bounds=[1, 1],
        method="sample",
        num_samples=100,
    )

    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=10000,
            species_size=20,
            survival_threshold=0.01,
            genome=DefaultGenome(
                num_inputs=2,
                num_outputs=1,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=[ACT.identity, ACT.inv, ACT.square],
                    aggregation_options=[AGG.sum, AGG.product],
                ),
                output_transform=ACT.identity,
            ),
        ),
        problem=custom_problem,
        generation_limit=50,
        fitness_target=-1e-4,
        seed=42,
    )

    # initialize state
    state = pipeline.setup()
    # run until terminate
    state, best = pipeline.auto_run(state)
    # show result
    pipeline.show(state, best)

=== ./examples/interpret_visualize/visualize_genome.py ===
h = np.zeros(3)
o = np.zeros(4)
h[0] = -tanh(0.540486*i[0] + 1.04397*i[1] + 0.58006*i[10] + 0.658223*i[11] - 0.9918*i[12] - 0.01919*i[13] + 0.194062*i[14] + 0.903314*i[15] - 1.906567*i[2] - 1.666336*i[3] + 0.653257*i[4] + 0.580191*i[5] + 0.177264*i[6] + 0.830688*i[7] - 0.855676*i[8] + 0.326538*i[9] + 2.465507)
h[1] = -tanh(1.441044*i[0] - 0.606109*i[1] - 0.736058*i[10] + 0.60264*i[11] - 0.837565*i[12] + 2.018719*i[13] + 0.327097*i[14] + 0.098963*i[15] + 0.403485*i[2] - 0.680547*i[3] + 0.349021*i[4] - 1.359364*i[5] + 0.351466*i[6] + 0.450447*i[7] + 2.102749*i[8] + 0.680605*i[9] + 0.593945)
h[2] = -tanh(1.350645*i[0] - 0.281682*i[1] + 0.332992*i[10] + 0.703457*i[11] + 1.290286*i[12] - 1.059887*i[13] - 1.114513*i[14] + 0.446127*i[15] + 1.103008*i[2] + 1.080698*i[3] - 0.89471*i[4] + 0.103146*i[5] - 0.828767*i[6] + 0.609362*i[7] - 0.765917*i[8] + 0.047898*i[9] + 0.649254)
o[0] = -1.307307*h[0] - 0.985838*h[1] - 0.746408*h[2] + 0.245725
o[1] = 0.64947*h[0] + 2.865669*h[1] + 1.185759*h[2] - 1.347174
o[2] = 2.030407*h[0] + 1.001914*h[1] - 1.041287*h[2] + 0.301639
o[3] = 0.717661*h[0] + 0.653905*h[1] - 1.387949*h[2] - 1.200779
=== ./examples/interpret_visualize/genome_sympy.py ===
import jax, jax.numpy as jnp

from tensorneat.genome import DefaultGenome
from tensorneat.common import *
from tensorneat.common.functions import SympySigmoid

if __name__ == "__main__":
    genome = DefaultGenome(
        num_inputs=3,
        num_outputs=1,
        max_nodes=50,
        max_conns=500,
        output_transform=ACT.sigmoid,
    )

    state = genome.setup()

    randkey = jax.random.PRNGKey(42)
    nodes, conns = genome.initialize(state, randkey)

    network = genome.network_dict(state, nodes, conns)

    input_idx, output_idx = genome.get_input_idx(), genome.get_output_idx()

    res = genome.sympy_func(state, network, sympy_input_transform=lambda x: 999*x, sympy_output_transform=SympySigmoid)
    (symbols,
    args_symbols,
    input_symbols,
    nodes_exprs,
    output_exprs,
    forward_func,) = res

    print(symbols)
    print(output_exprs[0].subs(args_symbols))

    inputs = jnp.zeros(3)
    print(forward_func(inputs))

    print(genome.forward(state, genome.transform(state, nodes, conns), inputs))

    print(AGG.sympy_module("jax"))
    print(AGG.sympy_module("numpy"))

    print(ACT.sympy_module("jax"))
    print(ACT.sympy_module("numpy"))
=== ./examples/gymnax/mountain_car_continuous.py ===
import jax.numpy as jnp

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode

from tensorneat.problem.rl import GymNaxEnv
from tensorneat.common import ACT, AGG



if __name__ == "__main__":
    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                num_inputs=2,
                num_outputs=1,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                ),
                output_transform=ACT.tanh,
            ),
        ),
        problem=GymNaxEnv(
            env_name="MountainCarContinuous-v0",
            repeat_times=5,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=99,
    )

    # initialize state
    state = pipeline.setup()

    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/gymnax/cartpole.py ===
import jax.numpy as jnp

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode

from tensorneat.problem.rl import GymNaxEnv
from tensorneat.common import ACT, AGG



if __name__ == "__main__":
    # the network has 2 outputs, the max one will be the action
    # as the action of cartpole is {0, 1}

    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                num_inputs=4,
                num_outputs=2,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                ),
                output_transform=jnp.argmax,
            ),
        ),
        problem=GymNaxEnv(
            env_name="CartPole-v1",
            repeat_times=5,
        ),
        seed=42,
        generation_limit=100,
        fitness_target=500,
    )

    # initialize state
    state = pipeline.setup()

    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/gymnax/cartpole_hyperneat.py ===
import jax.numpy as jnp

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.algorithm.hyperneat import HyperNEAT, FullSubstrate
from tensorneat.genome import DefaultGenome
from tensorneat.common import ACT

from tensorneat.problem import GymNaxEnv

if __name__ == "__main__":

    # the num of input_coors is 5
    # 4 is for cartpole inputs, 1 is for bias
    pipeline = Pipeline(
        algorithm=HyperNEAT(
            substrate=FullSubstrate(
                input_coors=((-1, -1), (-0.5, -1), (0, -1), (0.5, -1), (1, -1)),
                hidden_coors=((-1, 0), (0, 0), (1, 0)),
                output_coors=((-1, 1), (1, 1)),
            ),
            neat=NEAT(
                pop_size=10000,
                species_size=20,
                survival_threshold=0.01,
                genome=DefaultGenome(
                    num_inputs=4,  # size of query coors
                    num_outputs=1,
                    init_hidden_layers=(),
                    output_transform=ACT.tanh,
                ),
            ),
            activation=ACT.tanh,
            activate_time=10,
            output_transform=jnp.argmax,
        ),
        problem=GymNaxEnv(
            env_name="CartPole-v1",
            repeat_times=5,
        ),
        generation_limit=300,
        fitness_target=-1e-6,
    )

    # initialize state
    state = pipeline.setup()
    # print(state)
    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/gymnax/arcbot.py ===
import jax.numpy as jnp

from tensorneat.pipeline import Pipeline
from tensorneat.algorithm.neat import NEAT
from tensorneat.genome import DefaultGenome, BiasNode

from tensorneat.problem.rl import GymNaxEnv
from tensorneat.common import ACT, AGG



if __name__ == "__main__":
    # the network has 3 outputs, the max one will be the action
    # as the action of acrobot is {0, 1, 2}

    pipeline = Pipeline(
        algorithm=NEAT(
            pop_size=1000,
            species_size=20,
            survival_threshold=0.1,
            compatibility_threshold=1.0,
            genome=DefaultGenome(
                num_inputs=6,
                num_outputs=3,
                init_hidden_layers=(),
                node_gene=BiasNode(
                    activation_options=ACT.tanh,
                    aggregation_options=AGG.sum,
                ),
                output_transform=jnp.argmax,
            ),
        ),
        problem=GymNaxEnv(
            env_name="Acrobot-v1",
        ),
        seed=42,
        generation_limit=100,
        fitness_target=-60,
    )

    # initialize state
    state = pipeline.setup()

    # run until terminate
    state, best = pipeline.auto_run(state)

=== ./examples/with_evox/walker2d_evox.py ===
import jax
import jax.numpy as jnp

from evox import workflows, problems

from tensorneat.common.evox_adaptors import EvoXAlgorithmAdaptor, TensorNEATMonitor
from tensorneat.algorithm import NEAT
from tensorneat.genome import DefaultGenome, BiasNode
from tensorneat.common import ACT, AGG

neat_algorithm = NEAT(
    pop_size=1000,
    species_size=20,
    survival_threshold=0.1,
    compatibility_threshold=1.0,
    genome=DefaultGenome(
        max_nodes=50,
        max_conns=200,
        num_inputs=17,
        num_outputs=6,
        init_hidden_layers=(),
        node_gene=BiasNode(
            activation_options=ACT.tanh,
            aggregation_options=AGG.sum,
        ),
        output_transform=ACT.tanh,
    ),
)
evox_algorithm = EvoXAlgorithmAdaptor(neat_algorithm)

key = jax.random.PRNGKey(42)
model_key, workflow_key = jax.random.split(key)

monitor = TensorNEATMonitor(neat_algorithm, is_save=False)
problem = problems.neuroevolution.Brax(
    env_name="walker2d",
    policy=evox_algorithm.forward,
    max_episode_length=1000,
    num_episodes=1,
)


def nan2inf(x):
    return jnp.where(jnp.isnan(x), -jnp.inf, x)


# create a workflow
workflow = workflows.StdWorkflow(
    algorithm=evox_algorithm,
    problem=problem,
    candidate_transforms=[jax.jit(jax.vmap(evox_algorithm.transform))],
    fitness_transforms=[nan2inf],
    monitors=[monitor],
    opt_direction="max",
)

# init the workflow
state = workflow.init(workflow_key)

# enable multi devices
state = workflow.enable_multi_devices(state)

# run the workflow for 100 steps
for i in range(100):
    train_info, state = workflow.step(state)
    monitor.show()

=== ./src/tensorneat/problem/func_fit/custom.py ===
from typing import Callable, Union, List, Tuple
from jax import vmap, Array, numpy as jnp
import numpy as np

from .func_fit import FuncFit


class CustomFuncFit(FuncFit):

    def __init__(
        self,
        func: Callable,
        low_bounds: Union[List, Tuple, Array],
        upper_bounds: Union[List, Tuple, Array],
        method: str = "sample",
        num_samples: int = 100,
        step_size: Array = None,
        *args,
        **kwargs,
    ):

        if isinstance(low_bounds, list) or isinstance(low_bounds, tuple):
            low_bounds = np.array(low_bounds, dtype=np.float32)
        if isinstance(upper_bounds, list) or isinstance(upper_bounds, tuple):
            upper_bounds = np.array(upper_bounds, dtype=np.float32)

        try:
            out = func(low_bounds)
        except Exception as e:
            raise ValueError(f"func(low_bounds) raise an exception: {e}")
        assert low_bounds.shape == upper_bounds.shape

        assert method in {"sample", "grid"}

        self.func = func
        self.low_bounds = low_bounds
        self.upper_bounds = upper_bounds

        self.method = method
        self.num_samples = num_samples
        self.step_size = step_size

        self.generate_dataset()

        super().__init__(*args, **kwargs)

    def generate_dataset(self):

        if self.method == "sample":
            assert (
                self.num_samples > 0
            ), f"num_samples must be positive, got {self.num_samples}"

            inputs = np.zeros(
                (self.num_samples, self.low_bounds.shape[0]), dtype=np.float32
            )
            for i in range(self.low_bounds.shape[0]):
                inputs[:, i] = np.random.uniform(
                    low=self.low_bounds[i],
                    high=self.upper_bounds[i],
                    size=(self.num_samples,),
                )
        elif self.method == "grid":
            assert (
                self.step_size is not None
            ), "step_size must be provided when method is 'grid'"
            assert (
                self.step_size.shape == self.low_bounds.shape
            ), "step_size must have the same shape as low_bounds"
            assert np.all(self.step_size > 0), "step_size must be positive"

            inputs = np.zeros((1, 1))
            for i in range(self.low_bounds.shape[0]):
                new_col = np.arange(
                    self.low_bounds[i], self.upper_bounds[i], self.step_size[i]
                )
                inputs = cartesian_product(inputs, new_col[:, None])
            inputs = inputs[:, 1:]
        else:
            raise ValueError(f"Unknown method: {self.method}")

        outputs = vmap(self.func)(inputs)

        self.data_inputs = jnp.array(inputs)
        self.data_outputs = jnp.array(outputs)

    @property
    def inputs(self):
        return self.data_inputs

    @property
    def targets(self):
        return self.data_outputs

    @property
    def input_shape(self):
        return self.data_inputs.shape

    @property
    def output_shape(self):
        return self.data_outputs.shape


def cartesian_product(arr1, arr2):
    assert (
        arr1.ndim == arr2.ndim
    ), "arr1 and arr2 must have the same number of dimensions"
    assert arr1.ndim <= 2, "arr1 and arr2 must have at most 2 dimensions"

    len1 = arr1.shape[0]
    len2 = arr2.shape[0]

    repeated_arr1 = np.repeat(arr1, len2, axis=0)
    tiled_arr2 = np.tile(arr2, (len1, 1))

    new_arr = np.concatenate((repeated_arr1, tiled_arr2), axis=1)
    return new_arr

=== ./src/tensorneat/problem/func_fit/__init__.py ===
from .xor import XOR
from .xor3d import XOR3d
from .custom import CustomFuncFit
from .func_fit import FuncFit
=== ./src/tensorneat/problem/func_fit/xor3d.py ===
import numpy as np

from .func_fit import FuncFit


class XOR3d(FuncFit):
    @property
    def inputs(self):
        return np.array(
            [
                [0, 0, 0],
                [0, 0, 1],
                [0, 1, 0],
                [0, 1, 1],
                [1, 0, 0],
                [1, 0, 1],
                [1, 1, 0],
                [1, 1, 1],
            ],
            dtype=np.float32,
        )

    @property
    def targets(self):
        return np.array(
            [[0], [1], [1], [0], [1], [0], [0], [1]],
            dtype=np.float32,
        )

    @property
    def input_shape(self):
        return 8, 3

    @property
    def output_shape(self):
        return 8, 1

=== ./src/tensorneat/problem/func_fit/func_fit.py ===
import jax
from jax import vmap, numpy as jnp
import jax.numpy as jnp

from ..base import BaseProblem
from tensorneat.common import State


class FuncFit(BaseProblem):
    jitable = True

    def __init__(self, error_method: str = "mse"):
        super().__init__()

        assert error_method in {"mse", "rmse", "mae", "mape"}
        self.error_method = error_method

    def setup(self, state: State = State()):
        return state

    def evaluate(self, state, randkey, act_func, params):

        predict = vmap(act_func, in_axes=(None, None, 0))(
            state, params, self.inputs
        )

        if self.error_method == "mse":
            loss = jnp.mean((predict - self.targets) ** 2)

        elif self.error_method == "rmse":
            loss = jnp.sqrt(jnp.mean((predict - self.targets) ** 2))

        elif self.error_method == "mae":
            loss = jnp.mean(jnp.abs(predict - self.targets))

        elif self.error_method == "mape":
            loss = jnp.mean(jnp.abs((predict - self.targets) / self.targets))

        else:
            raise NotImplementedError

        return -loss

    def show(self, state, randkey, act_func, params, *args, **kwargs):
        predict = vmap(act_func, in_axes=(None, None, 0))(
            state, params, self.inputs
        )
        inputs, target, predict = jax.device_get([self.inputs, self.targets, predict])
        fitness = self.evaluate(state, randkey, act_func, params)

        loss = -fitness

        msg = ""
        for i in range(inputs.shape[0]):
            msg += f"input: {inputs[i]}, target: {target[i]}, predict: {predict[i]}\n"
        msg += f"loss: {loss}\n"
        print(msg)

    @property
    def inputs(self):
        raise NotImplementedError

    @property
    def targets(self):
        raise NotImplementedError

    @property
    def input_shape(self):
        raise NotImplementedError

    @property
    def output_shape(self):
        raise NotImplementedError

=== ./src/tensorneat/problem/func_fit/xor.py ===
import numpy as np

from .func_fit import FuncFit


class XOR(FuncFit):
    @property
    def inputs(self):
        return np.array(
            [[0, 0], [0, 1], [1, 0], [1, 1]],
            dtype=np.float32,
        )

    @property
    def targets(self):
        return np.array(
            [[0], [1], [1], [0]],
            dtype=np.float32,
        )

    @property
    def input_shape(self):
        return 4, 2

    @property
    def output_shape(self):
        return 4, 1

=== ./src/tensorneat/problem/__init__.py ===
from .base import BaseProblem
from .rl import *
from .func_fit import *

=== ./src/tensorneat/problem/base.py ===
from typing import Callable

from tensorneat.common import State, StatefulBaseClass


class BaseProblem(StatefulBaseClass):
    jitable = None

    def evaluate(self, state: State, randkey, act_func: Callable, params):
        """evaluate one individual"""
        raise NotImplementedError

    @property
    def input_shape(self):
        """
        The input shape for the problem to evaluate
        In RL problem, it is the observation space
        In function fitting problem, it is the input shape of the function
        """
        raise NotImplementedError

    @property
    def output_shape(self):
        """
        The output shape for the problem to evaluate
        In RL problem, it is the action space
        In function fitting problem, it is the output shape of the function
        """
        raise NotImplementedError

    def show(self, state: State, randkey, act_func: Callable, params, *args, **kwargs):
        """
        show how a genome perform in this problem
        """
        raise NotImplementedError

=== ./src/tensorneat/problem/rl/gymnax.py ===
import gymnax

from .rl_jit import RLEnv


class GymNaxEnv(RLEnv):
    def __init__(self, env_name, *args, **kwargs):
        super().__init__(*args, **kwargs)
        assert env_name in gymnax.registered_envs, f"Env {env_name} not registered in gymnax."
        self.env, self.env_params = gymnax.make(env_name)

    def env_step(self, randkey, env_state, action):
        return self.env.step(randkey, env_state, action, self.env_params)

    def env_reset(self, randkey):
        return self.env.reset(randkey, self.env_params)

    @property
    def input_shape(self):
        return self.env.observation_space(self.env_params).shape

    @property
    def output_shape(self):
        return self.env.action_space(self.env_params).shape

    def show(self, state, randkey, act_func, params, *args, **kwargs):
        raise NotImplementedError

=== ./src/tensorneat/problem/rl/__init__.py ===
from .gymnax import GymNaxEnv
from .brax import BraxEnv
from .rl_jit import RLEnv
=== ./src/tensorneat/problem/rl/brax.py ===
import jax.numpy as jnp
from brax import envs

from .rl_jit import RLEnv, norm_obs


class BraxEnv(RLEnv):
    def __init__(
        self, env_name: str = "ant", backend: str = "generalized", *args, **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.env_name = env_name
        self.env = envs.create(env_name=env_name, backend=backend)

    def env_step(self, randkey, env_state, action):
        state = self.env.step(env_state, action)
        return state.obs, state, state.reward, state.done.astype(jnp.bool_), state.info

    def env_reset(self, randkey):
        init_state = self.env.reset(randkey)
        return init_state.obs, init_state

    @property
    def input_shape(self):
        return (self.env.observation_size,)

    @property
    def output_shape(self):
        return (self.env.action_size,)

    def show(
            self,
            state,
            randkey,
            act_func,
            params,
            save_path=None,
            height=480,
            width=480,
            output_type="rgb_array",
            *args,
            **kwargs,
        ):
    
        assert output_type in ["rgb_array", "gif"]

        import jax
        import imageio
        from brax.io import image
        import numpy as np

        obs, env_state = self.reset(randkey)
        reward, done = 0.0, False
        state_histories = [env_state.pipeline_state]

        def step(key, env_state, obs):
            key, _ = jax.random.split(key)

            if self.obs_normalization:
                obs = norm_obs(state, obs)

            if self.action_policy is not None:
                forward_func = lambda obs: act_func(state, params, obs)
                action = self.action_policy(key, forward_func, obs)
            else:
                action = act_func(state, params, obs)

            obs, env_state, r, done, info = self.step(randkey, env_state, action)
            return key, env_state, obs, r, done

        jit_step = jax.jit(step)

        for _ in range(self.max_step):
            key, env_state, obs, r, done = jit_step(randkey, env_state, obs)
            state_histories.append(env_state.pipeline_state)
            reward += r
            if done:
                break
        
        print("Total reward: ", reward)

        imgs = image.render_array(
            sys=self.env.sys, trajectory=state_histories, height=height, width=width, camera="track"
        )

        if output_type == "rgb_array":
            imgs = np.array(imgs)
            return imgs

        if save_path is None:
            save_path = f"{self.env_name}.gif"

        imageio.mimsave(save_path, imgs, *args, **kwargs)

        print("Gif saved to: ", save_path)

=== ./src/tensorneat/problem/rl/rl_jit.py ===
from typing import Callable

import jax
from jax import vmap, numpy as jnp
import numpy as np

from ..base import BaseProblem
from tensorneat.common import State


class RLEnv(BaseProblem):
    jitable = True

    def __init__(
        self,
        max_step=1000,
        repeat_times=1,
        action_policy: Callable = None,
        obs_normalization: bool = False,
        sample_policy: Callable = None,
        sample_episodes: int = 0,
    ):
        """
        action_policy take three args:
            randkey, forward_func, obs
            randkey is a random key for jax.random
            forward_func is a function which receive obs and return action forward_func(obs) - > action
            obs is the observation of the environment

        sample_policy take two args:
            randkey, obs -> action
        """

        super().__init__()
        self.max_step = max_step
        self.repeat_times = repeat_times
        self.action_policy = action_policy

        if obs_normalization:
            assert sample_policy is not None, "sample_policy must be provided"
            assert sample_episodes > 0, "sample_size must be greater than 0"
            self.sample_policy = sample_policy
            self.sample_episodes = sample_episodes
        self.obs_normalization = obs_normalization

    def setup(self, state=State()):
        if self.obs_normalization:
            print("Sampling episodes for normalization")
            keys = jax.random.split(state.randkey, self.sample_episodes)
            dummy_act_func = (
                lambda s, p, o: o
            )  # receive state, params, obs and return the original obs
            dummy_sample_func = lambda rk, act_func, obs: self.sample_policy(
                rk, obs
            )  # ignore act_func

            def sample(rk):
                return self._evaluate_once(
                    state, rk, dummy_act_func, None, dummy_sample_func, True
                )

            rewards, episodes = jax.jit(vmap(sample))(keys)

            obs = jax.device_get(episodes["obs"])  # shape: (sample_episodes, max_step, *input_shape)
            obs = obs.reshape(
                -1, *self.input_shape
            )  # shape: (sample_episodes * max_step, *input_shape)

            obs_axis = tuple(range(obs.ndim))
            valid_data_flag = np.all(~jnp.isnan(obs), axis=obs_axis[1:])
            obs = obs[valid_data_flag]

            obs_mean = np.mean(obs, axis=0)
            obs_std = np.std(obs, axis=0)

            state = state.register(
                problem_obs_mean=obs_mean,
                problem_obs_std=obs_std,
            )

            print("Sampling episodes for normalization finished.")
            print("valid data count: ", obs.shape[0])
            print("obs_mean: ", obs_mean)
            print("obs_std: ", obs_std)
        return state

    def evaluate(self, state: State, randkey, act_func: Callable, params):
        keys = jax.random.split(randkey, self.repeat_times)
        rewards = vmap(
            self._evaluate_once, in_axes=(None, 0, None, None, None, None, None)
        )(
            state,
            keys,
            act_func,
            params,
            self.action_policy,
            False,
            self.obs_normalization,
        )

        return rewards.mean()

    def _evaluate_once(
        self,
        state,
        randkey,
        act_func,
        params,
        action_policy,
        record_episode,
        normalize_obs=False,
    ):
        rng_reset, rng_episode = jax.random.split(randkey)
        init_obs, init_env_state = self.reset(rng_reset)

        if record_episode:
            obs_array = jnp.full((self.max_step, *self.input_shape), jnp.nan)
            action_array = jnp.full((self.max_step, *self.output_shape), jnp.nan)
            reward_array = jnp.full((self.max_step,), jnp.nan)
            episode = {
                "obs": obs_array,
                "action": action_array,
                "reward": reward_array,
            }
        else:
            episode = None

        def cond_func(carry):
            _, _, _, done, _, count, _, rk = carry
            return ~done & (count < self.max_step)

        def body_func(carry):
            (
                obs,
                env_state,
                rng,
                done,
                tr,
                count,
                epis,
                rk,
            ) = carry  # tr -> total reward; rk -> randkey

            if normalize_obs:
                obs = norm_obs(state, obs)

            if action_policy is not None:
                forward_func = lambda obs: act_func(state, params, obs)
                action = action_policy(rk, forward_func, obs)
            else:
                action = act_func(state, params, obs)
            next_obs, next_env_state, reward, done, _ = self.step(
                rng, env_state, action
            )
            next_rng, _ = jax.random.split(rng)

            if record_episode:
                epis["obs"] = epis["obs"].at[count].set(obs)
                epis["action"] = epis["action"].at[count].set(action)
                epis["reward"] = epis["reward"].at[count].set(reward)

            return (
                next_obs,
                next_env_state,
                next_rng,
                done,
                tr + reward,
                count + 1,
                epis,
                jax.random.split(rk)[0],
            )

        _, _, _, _, total_reward, _, episode, _ = jax.lax.while_loop(
            cond_func,
            body_func,
            (init_obs, init_env_state, rng_episode, False, 0.0, 0, episode, randkey),
        )

        if record_episode:
            return total_reward, episode
        else:
            return total_reward

    def step(self, randkey, env_state, action):
        return self.env_step(randkey, env_state, action)

    def reset(self, randkey):
        return self.env_reset(randkey)

    def env_step(self, randkey, env_state, action):
        raise NotImplementedError

    def env_reset(self, randkey):
        raise NotImplementedError

    @property
    def input_shape(self):
        raise NotImplementedError

    @property
    def output_shape(self):
        raise NotImplementedError

    def show(self, state, randkey, act_func, params, *args, **kwargs):
        raise NotImplementedError


def norm_obs(state, obs):
    return (obs - state.problem_obs_mean) / (state.problem_obs_std + 1e-6)

=== ./src/tensorneat/algorithm/hyperneat/hyperneat.py ===
from typing import Callable

import jax
from jax import vmap, numpy as jnp

from .substrate import *
from ...common import State, ACT, AGG
from ...algorithm import BaseAlgorithm, NEAT
from ...genome import BaseNode, BaseConn, RecurrentGenome


class HyperNEAT(BaseAlgorithm):
    def __init__(
        self,
        substrate: BaseSubstrate,
        neat: NEAT,
        weight_threshold: float = 0.3,
        max_weight: float = 5.0,
        aggregation: Callable = AGG.sum,
        activation: Callable = ACT.sigmoid,
        activate_time: int = 10,
        output_transform: Callable = ACT.sigmoid,
    ):
        assert (
            substrate.query_coors.shape[1] == neat.num_inputs
        ), "Query coors of Substrate should be equal to NEAT input size"
        
        self.substrate = substrate
        self.neat = neat
        self.weight_threshold = weight_threshold
        self.max_weight = max_weight
        self.hyper_genome = RecurrentGenome(
            num_inputs=substrate.num_inputs,
            num_outputs=substrate.num_outputs,
            max_nodes=substrate.nodes_cnt,
            max_conns=substrate.conns_cnt,
            node_gene=HyperNEATNode(aggregation, activation),
            conn_gene=HyperNEATConn(),
            activate_time=activate_time,
            output_transform=output_transform,
        )
        self.pop_size = neat.pop_size

    def setup(self, state=State()):
        state = self.neat.setup(state)
        state = self.substrate.setup(state)
        return self.hyper_genome.setup(state)

    def ask(self, state):
        return self.neat.ask(state)

    def tell(self, state, fitness):
        state = self.neat.tell(state, fitness)
        return state

    def transform(self, state, individual):
        transformed = self.neat.transform(state, individual)
        query_res = vmap(self.neat.forward, in_axes=(None, None, 0))(
            state, transformed, self.substrate.query_coors
        )
        # mute the connection with weight weight threshold
        query_res = jnp.where(
            (-self.weight_threshold < query_res) & (query_res < self.weight_threshold),
            0.0,
            query_res,
        )

        # make query res in range [-max_weight, max_weight]
        query_res = jnp.where(
            query_res > 0, query_res - self.weight_threshold, query_res
        )
        query_res = jnp.where(
            query_res < 0, query_res + self.weight_threshold, query_res
        )
        query_res = query_res / (1 - self.weight_threshold) * self.max_weight

        h_nodes, h_conns = self.substrate.make_nodes(
            query_res
        ), self.substrate.make_conns(query_res)

        h_nodes, h_conns = jax.device_put([h_nodes, h_conns])

        return self.hyper_genome.transform(state, h_nodes, h_conns)

    def forward(self, state, transformed, inputs):
        # add bias
        inputs_with_bias = jnp.concatenate([inputs, jnp.array([1])])

        res = self.hyper_genome.forward(state, transformed, inputs_with_bias)
        return res

    @property
    def num_inputs(self):
        return self.substrate.num_inputs - 1  # remove bias

    @property
    def num_outputs(self):
        return self.substrate.num_outputs

    def show_details(self, state, fitness):
        return self.neat.show_details(state, fitness)


class HyperNEATNode(BaseNode):
    def __init__(
        self,
        aggregation=AGG.sum,
        activation=ACT.sigmoid,
    ):
        super().__init__()
        self.aggregation = aggregation
        self.activation = activation

    def forward(self, state, attrs, inputs, is_output_node=False):
        return jax.lax.cond(
            is_output_node,
            lambda: self.aggregation(inputs),  # output node does not need activation
            lambda: self.activation(self.aggregation(inputs)),
        )


class HyperNEATConn(BaseConn):
    custom_attrs = ["weight"]

    def forward(self, state, attrs, inputs):
        weight = attrs[0]
        return inputs * weight

=== ./src/tensorneat/algorithm/hyperneat/substrate/__init__.py ===
from .base import BaseSubstrate
from .default import DefaultSubstrate
from .full import FullSubstrate
from .mlp import MLPSubstrate
=== ./src/tensorneat/algorithm/hyperneat/substrate/mlp.py ===
from typing import List, Tuple
import numpy as np

from .default import DefaultSubstrate


class MLPSubstrate(DefaultSubstrate):

    connection_type = "feedforward"

    def __init__(self, layers: List[int], coor_range: Tuple[float] = (-1, 1, -1, 1)):
        """
        layers: list of integers, the number of neurons in each layer
        coor_range: tuple of 4 floats, the range of the substrate. (x_min, x_max, y_min, y_max)
        """
        assert len(layers) >= 2, "The number of layers should be at least 2"
        for layer in layers:
            assert layer > 0, "The number of neurons in each layer should be positive"
        assert coor_range[0] < coor_range[1], "x_min should be less than x_max"
        assert coor_range[2] < coor_range[3], "y_min should be less than y_max"

        num_inputs = layers[0]
        num_outputs = layers[-1]
        query_coors, nodes, conns = analysis_substrate(layers, coor_range)
        super().__init__(num_inputs, num_outputs, query_coors, nodes, conns)


def analysis_substrate(layers, coor_range):
    x_min, x_max, y_min, y_max = coor_range
    layer_cnt = len(layers)
    y_interval = (y_max - y_min) / (layer_cnt - 1)

    # prepare nodes indices and coordinates
    node_coors = {}
    input_indices = list(range(layers[0]))
    input_coors = cal_coors(layers[0], x_min, x_max, y_min)

    output_indices = list(range(layers[0], layers[0] + layers[-1]))
    output_coors = cal_coors(layers[-1], x_min, x_max, y_max)

    if layer_cnt == 2:  # only input and output layers
        node_layers = [input_indices, output_indices]
        node_coors = [*input_coors, *output_coors]
    else:
        hidden_indices, hidden_coors = [], []
        hidden_idx = layers[0] + layers[-1]
        hidden_layers = []
        for layer_idx in range(1, layer_cnt - 1):
            y_coor = y_min + layer_idx * y_interval
            indices = list(range(hidden_idx, hidden_idx + layers[layer_idx]))
            coors = cal_coors(layers[layer_idx], x_min, x_max, y_coor)

            hidden_layers.append(indices)
            hidden_indices.extend(indices)
            hidden_coors.extend(coors)
            hidden_idx += layers[layer_idx]

        node_layers = [
            input_indices,
            *hidden_layers,
            output_indices,
        ]  # the layers of hyperneat network
        node_coors = [*input_coors, *output_coors, *hidden_coors]

    # prepare connections
    query_coors, correspond_keys = [], []
    for layer_idx in range(layer_cnt - 1):
        for i in range(layers[layer_idx]):
            for j in range(layers[layer_idx + 1]):
                neuron1 = node_layers[layer_idx][i]
                neuron2 = node_layers[layer_idx + 1][j]
                query_coors.append((*node_coors[neuron1], *node_coors[neuron2]))
                correspond_keys.append((neuron1, neuron2))

    # nodes order in TensorNEAT must be input->output->hidden
    ordered_nodes = [*node_layers[0], *node_layers[-1]]
    for layer in node_layers[1:-1]:
        ordered_nodes.extend(layer)
    nodes = np.array(ordered_nodes)[:, np.newaxis]
    conns = np.zeros(
        (len(correspond_keys), 3), dtype=np.float32
    )  # input_idx, output_idx, weight
    conns[:, :2] = correspond_keys

    query_coors = np.array(query_coors)

    return query_coors, nodes, conns


def cal_coors(neuron_cnt, x_min, x_max, y_coor):
    if neuron_cnt == 1:  # only one neuron in this layer
        return [((x_min + x_max) / 2, y_coor)]
    x_interval = (x_max - x_min) / (neuron_cnt - 1)
    return [(x_min + x_interval * i, y_coor) for i in range(neuron_cnt)]

=== ./src/tensorneat/algorithm/hyperneat/substrate/full.py ===
import numpy as np
from .default import DefaultSubstrate


class FullSubstrate(DefaultSubstrate):

    connection_type = "recurrent"

    def __init__(
        self,
        input_coors=((-1, -1), (0, -1), (1, -1)),
        hidden_coors=((-1, 0), (0, 0), (1, 0)),
        output_coors=((0, 1),),
    ):
        query_coors, nodes, conns = analysis_substrate(
            input_coors, output_coors, hidden_coors
        )
        super().__init__(len(input_coors), len(output_coors), query_coors, nodes, conns)


def analysis_substrate(input_coors, output_coors, hidden_coors):
    input_coors = np.array(input_coors)
    output_coors = np.array(output_coors)
    hidden_coors = np.array(hidden_coors)

    cd = input_coors.shape[1]  # coordinate dimensions
    si = input_coors.shape[0]  # input coordinate size
    so = output_coors.shape[0]  # output coordinate size
    sh = hidden_coors.shape[0]  # hidden coordinate size

    input_idx = np.arange(si)
    output_idx = np.arange(si, si + so)
    hidden_idx = np.arange(si + so, si + so + sh)

    total_conns = si * sh + sh * sh + sh * so
    query_coors = np.zeros((total_conns, cd * 2))
    correspond_keys = np.zeros((total_conns, 2))

    # connect input to hidden
    aux_coors, aux_keys = cartesian_product(
        input_idx, hidden_idx, input_coors, hidden_coors
    )
    query_coors[0 : si * sh, :] = aux_coors
    correspond_keys[0 : si * sh, :] = aux_keys

    # connect hidden to hidden
    aux_coors, aux_keys = cartesian_product(
        hidden_idx, hidden_idx, hidden_coors, hidden_coors
    )
    query_coors[si * sh : si * sh + sh * sh, :] = aux_coors
    correspond_keys[si * sh : si * sh + sh * sh, :] = aux_keys

    # connect hidden to output
    aux_coors, aux_keys = cartesian_product(
        hidden_idx, output_idx, hidden_coors, output_coors
    )
    query_coors[si * sh + sh * sh :, :] = aux_coors
    correspond_keys[si * sh + sh * sh :, :] = aux_keys

    nodes = np.concatenate((input_idx, output_idx, hidden_idx))[..., np.newaxis]
    conns = np.zeros(
        (correspond_keys.shape[0], 3), dtype=np.float32
    )  # input_idx, output_idx, weight
    conns[:, :2] = correspond_keys

    print(query_coors, nodes, conns)
    return query_coors, nodes, conns


def cartesian_product(keys1, keys2, coors1, coors2):
    len1 = keys1.shape[0]
    len2 = keys2.shape[0]

    repeated_coors1 = np.repeat(coors1, len2, axis=0)
    repeated_keys1 = np.repeat(keys1, len2)

    tiled_coors2 = np.tile(coors2, (len1, 1))
    tiled_keys2 = np.tile(keys2, len1)

    new_coors = np.concatenate((repeated_coors1, tiled_coors2), axis=1)
    correspond_keys = np.column_stack((repeated_keys1, tiled_keys2))

    return new_coors, correspond_keys

=== ./src/tensorneat/algorithm/hyperneat/substrate/default.py ===
from jax import vmap
import numpy as np

from .base import BaseSubstrate
from ....genome.utils import set_gene_attrs


class DefaultSubstrate(BaseSubstrate):

    connection_type = "recurrent"

    def __init__(self, num_inputs, num_outputs, coors, nodes, conns):
        self.inputs = num_inputs
        self.outputs = num_outputs
        self.coors = np.array(coors)
        self.nodes = np.array(nodes)
        self.conns = np.array(conns)

    def make_nodes(self, query_res):
        return self.nodes

    def make_conns(self, query_res):
        # change weight of conns
        # the last column is the weight
        return self.conns.at[:, -1].set(query_res)

    @property
    def query_coors(self):
        return self.coors

    @property
    def num_inputs(self):
        return self.inputs

    @property
    def num_outputs(self):
        return self.outputs

    @property
    def nodes_cnt(self):
        return self.nodes.shape[0]

    @property
    def conns_cnt(self):
        return self.conns.shape[0]

=== ./src/tensorneat/algorithm/hyperneat/substrate/base.py ===
from ....common import StatefulBaseClass


class BaseSubstrate(StatefulBaseClass):

    connection_type = None

    def make_nodes(self, query_res):
        raise NotImplementedError

    def make_conns(self, query_res):
        raise NotImplementedError

    @property
    def query_coors(self):
        raise NotImplementedError

    @property
    def num_inputs(self):
        raise NotImplementedError

    @property
    def num_outputs(self):
        raise NotImplementedError

    @property
    def nodes_cnt(self):
        raise NotImplementedError

    @property
    def conns_cnt(self):
        raise NotImplementedError

=== ./src/tensorneat/algorithm/hyperneat/__init__.py ===
from .hyperneat import HyperNEAT
from .hyperneat_feedforward import HyperNEATFeedForward
from .substrate import BaseSubstrate, DefaultSubstrate, FullSubstrate, MLPSubstrate


=== ./src/tensorneat/algorithm/hyperneat/hyperneat_feedforward.py ===
"""
HyperNEAT with Feedforward Substrate and genome
"""

from typing import Callable

from .substrate import *
from .hyperneat import HyperNEAT, HyperNEATNode, HyperNEATConn
from ...common import ACT, AGG
from ...algorithm import NEAT
from ...genome import DefaultGenome


class HyperNEATFeedForward(HyperNEAT):
    def __init__(
        self,
        substrate: BaseSubstrate,
        neat: NEAT,
        weight_threshold: float = 0.3,
        max_weight: float = 5.0,
        aggregation: Callable = AGG.sum,
        activation: Callable = ACT.sigmoid,
        output_transform: Callable = ACT.sigmoid,
    ):
        assert (
            substrate.query_coors.shape[1] == neat.num_inputs
        ), "Query coors of Substrate should be equal to NEAT input size"
        
        assert substrate.connection_type == "feedforward", "Substrate should be feedforward"

        self.substrate = substrate
        self.neat = neat
        self.weight_threshold = weight_threshold
        self.max_weight = max_weight
        self.hyper_genome = DefaultGenome(
            num_inputs=substrate.num_inputs,
            num_outputs=substrate.num_outputs,
            max_nodes=substrate.nodes_cnt,
            max_conns=substrate.conns_cnt,
            node_gene=HyperNEATNode(aggregation, activation),
            conn_gene=HyperNEATConn(),
            output_transform=output_transform,
        )
        self.pop_size = neat.pop_size

=== ./src/tensorneat/algorithm/__init__.py ===
from .base import BaseAlgorithm
from .neat import NEAT
from .hyperneat import HyperNEAT

=== ./src/tensorneat/algorithm/neat/neat.py ===
from typing import Callable

import jax
from jax import vmap, numpy as jnp
import numpy as np

from .species import SpeciesController
from .. import BaseAlgorithm
from ...common import State
from ...genome import BaseGenome


class NEAT(BaseAlgorithm):
    def __init__(
        self,
        genome: BaseGenome,
        pop_size: int,
        species_size: int = 10,
        max_stagnation: int = 15,
        species_elitism: int = 2,
        spawn_number_change_rate: float = 0.5,
        genome_elitism: int = 2,
        survival_threshold: float = 0.1,
        min_species_size: int = 1,
        compatibility_threshold: float = 2.0,
        species_fitness_func: Callable = jnp.max,
        species_number_calculate_by: str = "rank",
    ):

        assert species_number_calculate_by in [
            "rank",
            "fitness",
        ], "species_number_calculate_by should be either 'rank' or 'fitness'"

        self.genome = genome
        self.pop_size = pop_size
        self.species_controller = SpeciesController(
            pop_size,
            species_size,
            max_stagnation,
            species_elitism,
            spawn_number_change_rate,
            genome_elitism,
            survival_threshold,
            min_species_size,
            compatibility_threshold,
            species_fitness_func,
            species_number_calculate_by,
        )

    def setup(self, state=State()):
        # setup state
        state = self.genome.setup(state)

        k1, randkey = jax.random.split(state.randkey, 2)

        # initialize the population
        initialize_keys = jax.random.split(k1, self.pop_size)
        pop_nodes, pop_conns = vmap(self.genome.initialize, in_axes=(None, 0))(
            state, initialize_keys
        )

        state = state.register(
            pop_nodes=pop_nodes,
            pop_conns=pop_conns,
            generation=jnp.float32(0),
        )

        # initialize species state
        state = self.species_controller.setup(state, pop_nodes[0], pop_conns[0])

        return state.update(randkey=randkey)

    def ask(self, state):
        return state.pop_nodes, state.pop_conns

    def tell(self, state, fitness):
        state = state.update(generation=state.generation + 1)

        # tell fitness to species controller
        state, winner, loser, elite_mask = self.species_controller.update_species(
            state,
            fitness,
        )

        # create next population
        state = self._create_next_generation(state, winner, loser, elite_mask)

        # speciate the next population
        state = self.species_controller.speciate(state, self.genome.execute_distance)

        return state

    def transform(self, state, individual):
        nodes, conns = individual
        return self.genome.transform(state, nodes, conns)

    def forward(self, state, transformed, inputs):
        return self.genome.forward(state, transformed, inputs)

    @property
    def num_inputs(self):
        return self.genome.num_inputs

    @property
    def num_outputs(self):
        return self.genome.num_outputs

    def _create_next_generation(self, state, winner, loser, elite_mask):

        # find next node key for mutation
        all_nodes_keys = state.pop_nodes[:, :, 0]
        max_node_key = jnp.max(
            all_nodes_keys, where=~jnp.isnan(all_nodes_keys), initial=0
        )
        next_node_key = max_node_key + 1
        new_node_keys = jnp.arange(self.pop_size) + next_node_key

        # find next conn historical markers for mutation if needed
        if "historical_marker" in self.genome.conn_gene.fixed_attrs:
            all_conns_markers = vmap(
                self.genome.conn_gene.get_historical_marker, in_axes=(None, 0)
            )(state, state.pop_conns)

            max_conn_markers = jnp.max(
                all_conns_markers, where=~jnp.isnan(all_conns_markers), initial=0
            )
            next_conn_markers = max_conn_markers + 1
            new_conn_markers = (
                jnp.arange(self.pop_size * 3).reshape(self.pop_size, 3)
                + next_conn_markers
            )
        else:
            # no need to generate new conn historical markers
            # use 0
            new_conn_markers = jnp.full((self.pop_size, 3), 0)

        # prepare random keys
        k1, k2, randkey = jax.random.split(state.randkey, 3)
        crossover_randkeys = jax.random.split(k1, self.pop_size)
        mutate_randkeys = jax.random.split(k2, self.pop_size)

        wpn, wpc = state.pop_nodes[winner], state.pop_conns[winner]
        lpn, lpc = state.pop_nodes[loser], state.pop_conns[loser]

        # batch crossover
        n_nodes, n_conns = vmap(
            self.genome.execute_crossover, in_axes=(None, 0, 0, 0, 0, 0)
        )(
            state, crossover_randkeys, wpn, wpc, lpn, lpc
        )  # new_nodes, new_conns

        # batch mutation
        m_n_nodes, m_n_conns = vmap(
            self.genome.execute_mutation, in_axes=(None, 0, 0, 0, 0, 0)
        )(
            state, mutate_randkeys, n_nodes, n_conns, new_node_keys, new_conn_markers
        )  # mutated_new_nodes, mutated_new_conns

        # elitism don't mutate
        pop_nodes = jnp.where(elite_mask[:, None, None], n_nodes, m_n_nodes)
        pop_conns = jnp.where(elite_mask[:, None, None], n_conns, m_n_conns)

        return state.update(
            randkey=randkey,
            pop_nodes=pop_nodes,
            pop_conns=pop_conns,
        )

    def show_details(self, state, fitness):
        member_count = jax.device_get(state.species.member_count)
        species_sizes = [int(i) for i in member_count if i > 0]

        pop_nodes, pop_conns = jax.device_get([state.pop_nodes, state.pop_conns])
        nodes_cnt = (~np.isnan(pop_nodes[:, :, 0])).sum(axis=1)  # (P,)
        conns_cnt = (~np.isnan(pop_conns[:, :, 0])).sum(axis=1)  # (P,)

        max_node_cnt, min_node_cnt, mean_node_cnt = (
            max(nodes_cnt),
            min(nodes_cnt),
            np.mean(nodes_cnt),
        )

        max_conn_cnt, min_conn_cnt, mean_conn_cnt = (
            max(conns_cnt),
            min(conns_cnt),
            np.mean(conns_cnt),
        )

        print(
            f"\tnode counts: max: {max_node_cnt}, min: {min_node_cnt}, mean: {mean_node_cnt:.2f}\n",
            f"\tconn counts: max: {max_conn_cnt}, min: {min_conn_cnt}, mean: {mean_conn_cnt:.2f}\n",
            f"\tspecies: {len(species_sizes)}, {species_sizes}\n",
        )

=== ./src/tensorneat/algorithm/neat/species.py ===
from typing import Callable

import jax
from jax import vmap, numpy as jnp
import numpy as np

from ...common import (
    State,
    StatefulBaseClass,
    rank_elements,
    argmin_with_mask,
    fetch_first,
)


class SpeciesController(StatefulBaseClass):
    def __init__(
        self,
        pop_size,
        species_size,
        max_stagnation,
        species_elitism,
        spawn_number_change_rate,
        genome_elitism,
        survival_threshold,
        min_species_size,
        compatibility_threshold,
        species_fitness_func,
        species_number_calculate_by,
    ):
        self.pop_size = pop_size
        self.species_size = species_size
        self.species_arange = np.arange(self.species_size)
        self.max_stagnation = max_stagnation
        self.species_elitism = species_elitism
        self.spawn_number_change_rate = spawn_number_change_rate
        self.genome_elitism = genome_elitism
        self.survival_threshold = survival_threshold
        self.min_species_size = min_species_size
        self.compatibility_threshold = compatibility_threshold
        self.species_fitness_func = species_fitness_func
        self.species_number_calculate_by = species_number_calculate_by

    def setup(self, state, first_nodes, first_conns):
        # the unique index (primary key) for each species
        species_keys = jnp.full((self.species_size,), jnp.nan)

        # the best fitness of each species
        best_fitness = jnp.full((self.species_size,), jnp.nan)

        # the last 1 that the species improved
        last_improved = jnp.full((self.species_size,), jnp.nan)

        # the number of members of each species
        member_count = jnp.full((self.species_size,), jnp.nan)

        # the species index of each individual
        idx2species = jnp.zeros(self.pop_size)

        # nodes for each center genome of each species
        center_nodes = jnp.full(
            (self.species_size, *first_nodes.shape),
            jnp.nan,
        )

        # connections for each center genome of each species
        center_conns = jnp.full(
            (self.species_size, *first_conns.shape),
            jnp.nan,
        )

        species_keys = species_keys.at[0].set(0)
        best_fitness = best_fitness.at[0].set(-jnp.inf)
        last_improved = last_improved.at[0].set(0)
        member_count = member_count.at[0].set(self.pop_size)
        center_nodes = center_nodes.at[0].set(first_nodes)
        center_conns = center_conns.at[0].set(first_conns)

        species_state = State(
            species_keys=species_keys,
            best_fitness=best_fitness,
            last_improved=last_improved,
            member_count=member_count,
            idx2species=idx2species,
            center_nodes=center_nodes,
            center_conns=center_conns,
            next_species_key=jnp.float32(1),  # 0 is reserved for the first species
        )

        return state.register(species=species_state)

    def update_species(self, state, fitness):
        species_state = state.species

        # update the fitness of each species
        species_fitness = self._update_species_fitness(species_state, fitness)

        # stagnation species
        species_state, species_fitness = self._stagnation(
            species_state, species_fitness, state.generation
        )

        # sort species_info by their fitness. (also push nan to the end)
        sort_indices = jnp.argsort(species_fitness)[::-1]  # fitness from high to low

        species_state = species_state.update(
            species_keys=species_state.species_keys[sort_indices],
            best_fitness=species_state.best_fitness[sort_indices],
            last_improved=species_state.last_improved[sort_indices],
            member_count=species_state.member_count[sort_indices],
            center_nodes=species_state.center_nodes[sort_indices],
            center_conns=species_state.center_conns[sort_indices],
        )

        # decide the number of members of each species by their fitness
        if self.species_number_calculate_by == "rank":
            spawn_number = self._cal_spawn_numbers_by_rank(species_state)
        elif self.species_number_calculate_by == "fitness":
            spawn_number = self._cal_spawn_numbers_by_fitness(species_state)
        else:
            raise ValueError("species_number_calculate_by must be 'rank' or 'fitness'")

        k1, k2 = jax.random.split(state.randkey)
        # crossover info
        winner, loser, elite_mask = self._create_crossover_pair(
            species_state, k1, spawn_number, fitness
        )

        return (
            state.update(randkey=k2, species=species_state),
            winner,
            loser,
            elite_mask,
        )

    def _update_species_fitness(self, species_state, fitness):
        """
        obtain the fitness of the species by the fitness of each individual.
        use max criterion.
        """

        def aux_func(idx):
            s_fitness = jnp.where(
                species_state.idx2species == species_state.species_keys[idx],
                fitness,
                -jnp.inf,
            )
            val = self.species_fitness_func(s_fitness)
            return val

        return vmap(aux_func)(self.species_arange)

    def _stagnation(self, species_state, species_fitness, generation):
        """
        stagnation species.
        those species whose fitness is not better than the best fitness of the species for a long time will be stagnation.
        elitism species never stagnation
        """

        def check_stagnation(idx):
            # determine whether the species stagnation

            # not better than the best fitness of the species
            # for a long time
            st = (species_fitness[idx] <= species_state.best_fitness[idx]) & (
                generation - species_state.last_improved[idx] > self.max_stagnation
            )

            # update last_improved and best_fitness
            # whether better than the best fitness of the species
            li, bf = jax.lax.cond(
                species_fitness[idx] > species_state.best_fitness[idx],
                lambda: (generation, species_fitness[idx]),  # update
                lambda: (
                    species_state.last_improved[idx],
                    species_state.best_fitness[idx],
                ),  # not update
            )

            return st, bf, li

        spe_st, best_fitness, last_improved = vmap(check_stagnation)(
            self.species_arange
        )

        # update species state
        species_state = species_state.update(
            best_fitness=best_fitness,
            last_improved=last_improved,
        )

        # elite species will not be stagnation
        species_rank = rank_elements(species_fitness)
        spe_st = jnp.where(
            species_rank < self.species_elitism, False, spe_st
        )  # elitism never stagnation

        # set stagnation species to nan
        def update_func(idx):
            return jax.lax.cond(
                spe_st[idx],
                lambda: (
                    jnp.nan,  # species_key
                    jnp.nan,  # best_fitness
                    jnp.nan,  # last_improved
                    jnp.nan,  # member_count
                    jnp.full_like(species_state.center_nodes[idx], jnp.nan),
                    jnp.full_like(species_state.center_conns[idx], jnp.nan),
                    -jnp.inf,  # species_fitness
                ),  # stagnation species
                lambda: (
                    species_state.species_keys[idx],
                    species_state.best_fitness[idx],
                    species_state.last_improved[idx],
                    species_state.member_count[idx],
                    species_state.center_nodes[idx],
                    species_state.center_conns[idx],
                    species_fitness[idx],
                ),  # not stagnation species
            )

        (
            species_keys,
            best_fitness,
            last_improved,
            member_count,
            center_nodes,
            center_conns,
            species_fitness,
        ) = vmap(update_func)(self.species_arange)

        return (
            species_state.update(
                species_keys=species_keys,
                best_fitness=best_fitness,
                last_improved=last_improved,
                member_count=member_count,
                center_nodes=center_nodes,
                center_conns=center_conns,
            ),
            species_fitness,
        )

    def _cal_spawn_numbers_by_rank(self, species_state):
        """
        decide the number of members of each species by their fitness rank.
        the species with higher fitness will have more members
        Linear ranking selection
            e.g. N = 3, P=10 -> probability = [0.5, 0.33, 0.17], spawn_number = [5, 3, 2]
        """

        species_keys = species_state.species_keys

        is_species_valid = ~jnp.isnan(species_keys)
        valid_species_num = jnp.sum(is_species_valid)
        denominator = (
            (valid_species_num + 1) * valid_species_num / 2
        )  # obtain 3 + 2 + 1 = 6

        # calculate the spawn number rate by the rank of each species
        rank_score = valid_species_num - self.species_arange  # obtain [3, 2, 1]
        spawn_number_rate = rank_score / denominator  # obtain [0.5, 0.33, 0.17]

        target_spawn_number = jnp.floor(
            spawn_number_rate * self.pop_size
        )  # calculate member

        # Avoid too much variation of numbers for a species
        previous_size = species_state.member_count
        spawn_number = (
            previous_size
            + (target_spawn_number - previous_size) * self.spawn_number_change_rate
        )

        # maintain min_species_size, this will not influence nan
        spawn_number = jnp.where(
            spawn_number < self.min_species_size, self.min_species_size, spawn_number
        )
        # convert to int, this will also make nan to 0
        spawn_number = spawn_number.astype(jnp.int32)

        # must control the sum of spawn_number to be equal to pop_size
        error = self.pop_size - jnp.sum(spawn_number)

        # add error to the first species to control the sum of spawn_number
        spawn_number = spawn_number.at[0].add(error)

        return spawn_number

    def _cal_spawn_numbers_by_fitness(self, species_state):
        """
        decide the number of members of each species by their fitness.
        the species with higher fitness will have more members
        """

        # the fitness of each species
        species_fitness = species_state.best_fitness

        # normalize the fitness before calculating the spawn number
        # consider that the fitness may be negative
        # in this way the species with the lowest fitness will have spawn_number = 0
        species_fitness = species_fitness - jnp.min(species_fitness)

        # calculate the spawn number rate by the fitness of each species
        spawn_number_rate = species_fitness / jnp.sum(
            species_fitness, where=~jnp.isnan(species_fitness)
        )
        target_spawn_number = jnp.floor(
            spawn_number_rate * self.pop_size
        )  # calculate member

        # Avoid too much variation of numbers for a species
        previous_size = species_state.member_count
        spawn_number = (
            previous_size
            + (target_spawn_number - previous_size) * self.spawn_number_change_rate
        )
        # maintain min_species_size, this will not influence nan
        spawn_number = jnp.where(
            spawn_number < self.min_species_size, self.min_species_size, spawn_number
        )

        # convert to int, this will also make nan to 0
        spawn_number = spawn_number.astype(jnp.int32)

        # must control the sum of spawn_number to be equal to pop_size
        error = self.pop_size - jnp.sum(spawn_number)

        # add error to the first species to control the sum of spawn_number
        spawn_number = spawn_number.at[0].add(error)

        return spawn_number

    def _create_crossover_pair(self, species_state, randkey, spawn_number, fitness):
        s_idx = self.species_arange
        p_idx = jnp.arange(self.pop_size)

        def aux_func(key, idx):
            # choose parents from the in the same species
            # key -> randkey, idx -> the idx of current species

            members = species_state.idx2species == species_state.species_keys[idx]
            members_num = jnp.sum(members)

            members_fitness = jnp.where(members, fitness, -jnp.inf)
            sorted_member_indices = jnp.argsort(members_fitness)[::-1]

            survive_size = jnp.floor(self.survival_threshold * members_num).astype(
                jnp.int32
            )

            select_pro = (p_idx < survive_size) / survive_size
            fa, ma = jax.random.choice(
                key,
                sorted_member_indices,
                shape=(2, self.pop_size),
                replace=True,
                p=select_pro,
            )

            # elite
            fa = jnp.where(p_idx < self.genome_elitism, sorted_member_indices, fa)
            ma = jnp.where(p_idx < self.genome_elitism, sorted_member_indices, ma)
            elite = jnp.where(p_idx < self.genome_elitism, True, False)
            return fa, ma, elite

        # choose parents to crossover in each species
        # fas, mas, elites: (self.species_size, self.pop_size)
        # fas -> father indices, mas -> mother indices, elites -> whether elite or not
        fas, mas, elites = vmap(aux_func)(
            jax.random.split(randkey, self.species_size), s_idx
        )

        # merge choosen parents from each species into one array
        # winner, loser, elite_mask: (self.pop_size)
        # winner -> winner indices, loser -> loser indices, elite_mask -> whether elite or not
        spawn_number_cum = jnp.cumsum(spawn_number)

        def aux_func(idx):
            loc = jnp.argmax(idx < spawn_number_cum)

            # elite genomes are at the beginning of the species
            idx_in_species = jnp.where(loc > 0, idx - spawn_number_cum[loc - 1], idx)
            return (
                fas[loc, idx_in_species],
                mas[loc, idx_in_species],
                elites[loc, idx_in_species],
            )

        part1, part2, elite_mask = vmap(aux_func)(p_idx)

        is_part1_win = fitness[part1] >= fitness[part2]
        winner = jnp.where(is_part1_win, part1, part2)
        loser = jnp.where(is_part1_win, part2, part1)

        return winner, loser, elite_mask

    def speciate(self, state, genome_distance_func: Callable):
        # prepare distance functions
        o2p_distance_func = vmap(
            genome_distance_func, in_axes=(None, None, None, 0, 0)
        )  # one to population

        # idx to specie key
        idx2species = jnp.full(
            (self.pop_size,), jnp.nan
        )  # NaN means not assigned to any species

        # the distance between genomes to its center genomes
        o2c_distances = jnp.full((self.pop_size,), jnp.inf)

        # step 1: find new centers
        def cond_func(carry):
            # i, idx2species, center_nodes, center_conns, o2c_distances
            i, i2s, cns, ccs, o2c = carry

            return (i < self.species_size) & (
                ~jnp.isnan(state.species.species_keys[i])
            )  # current species is existing

        def body_func(carry):
            i, i2s, cns, ccs, o2c = carry

            distances = o2p_distance_func(
                state, cns[i], ccs[i], state.pop_nodes, state.pop_conns
            )

            # find the closest one
            closest_idx = argmin_with_mask(distances, mask=jnp.isnan(i2s))

            i2s = i2s.at[closest_idx].set(state.species.species_keys[i])
            cns = cns.at[i].set(state.pop_nodes[closest_idx])
            ccs = ccs.at[i].set(state.pop_conns[closest_idx])

            # the genome with closest_idx will become the new center, thus its distance to center is 0.
            o2c = o2c.at[closest_idx].set(0)

            return i + 1, i2s, cns, ccs, o2c

        _, idx2species, center_nodes, center_conns, o2c_distances = jax.lax.while_loop(
            cond_func,
            body_func,
            (
                0,
                idx2species,
                state.species.center_nodes,
                state.species.center_conns,
                o2c_distances,
            ),
        )

        state = state.update(
            species=state.species.update(
                idx2species=idx2species,
                center_nodes=center_nodes,
                center_conns=center_conns,
            ),
        )

        # part 2: assign members to each species
        def cond_func(carry):
            # i, idx2species, center_nodes, center_conns, species_keys, o2c_distances, next_species_key
            i, i2s, cns, ccs, sk, o2c, nsk = carry

            current_species_existed = ~jnp.isnan(sk[i])
            not_all_assigned = jnp.any(jnp.isnan(i2s))
            not_reach_species_upper_bounds = i < self.species_size
            return not_reach_species_upper_bounds & (
                current_species_existed | not_all_assigned
            )

        def body_func(carry):
            i, i2s, cns, ccs, sk, o2c, nsk = carry

            _, i2s, cns, ccs, sk, o2c, nsk = jax.lax.cond(
                jnp.isnan(sk[i]),  # whether the current species is existing or not
                create_new_species,  # if not existing, create a new specie
                update_exist_specie,  # if existing, update the specie
                (i, i2s, cns, ccs, sk, o2c, nsk),
            )

            return i + 1, i2s, cns, ccs, sk, o2c, nsk

        def create_new_species(carry):
            i, i2s, cns, ccs, sk, o2c, nsk = carry

            # pick the first one who has not been assigned to any species
            idx = fetch_first(jnp.isnan(i2s))

            # assign it to the new species
            # [key, best score, last update generation, member_count]
            sk = sk.at[i].set(nsk)  # nsk -> next species key
            i2s = i2s.at[idx].set(nsk)
            o2c = o2c.at[idx].set(0)

            # update center genomes
            cns = cns.at[i].set(state.pop_nodes[idx])
            ccs = ccs.at[i].set(state.pop_conns[idx])

            # find the members for the new species
            i2s, o2c = speciate_by_threshold(i, i2s, cns, ccs, sk, o2c)

            return i, i2s, cns, ccs, sk, o2c, nsk + 1  # change to next new speciate key

        def update_exist_specie(carry):
            i, i2s, cns, ccs, sk, o2c, nsk = carry

            i2s, o2c = speciate_by_threshold(i, i2s, cns, ccs, sk, o2c)

            # turn to next species
            return i + 1, i2s, cns, ccs, sk, o2c, nsk

        def speciate_by_threshold(i, i2s, cns, ccs, sk, o2c):
            # distance between such center genome and ppo genomes
            o2p_distance = o2p_distance_func(
                state, cns[i], ccs[i], state.pop_nodes, state.pop_conns
            )

            close_enough_mask = o2p_distance < self.compatibility_threshold
            # when a genome is not assigned or the distance between its current center is bigger than this center
            catchable_mask = jnp.isnan(i2s) | (o2p_distance < o2c)

            mask = close_enough_mask & catchable_mask

            # update species info
            i2s = jnp.where(mask, sk[i], i2s)

            # update distance between centers
            o2c = jnp.where(mask, o2p_distance, o2c)

            return i2s, o2c

        # update idx2species
        (
            _,
            idx2species,
            center_nodes,
            center_conns,
            species_keys,
            _,
            next_species_key,
        ) = jax.lax.while_loop(
            cond_func,
            body_func,
            (
                0,
                state.species.idx2species,
                center_nodes,
                center_conns,
                state.species.species_keys,
                o2c_distances,
                state.species.next_species_key,
            ),
        )

        # if there are still some pop genomes not assigned to any species, add them to the last genome
        # this condition can only happen when the number of species is reached species upper bounds
        idx2species = jnp.where(jnp.isnan(idx2species), species_keys[-1], idx2species)

        # complete info of species which is created in this generation
        new_created_mask = (~jnp.isnan(species_keys)) & jnp.isnan(
            state.species.best_fitness
        )
        best_fitness = jnp.where(new_created_mask, -jnp.inf, state.species.best_fitness)
        last_improved = jnp.where(
            new_created_mask, state.generation, state.species.last_improved
        )

        # update members count
        def count_members(idx):
            return jax.lax.cond(
                jnp.isnan(species_keys[idx]),  # if the species is not existing
                lambda: jnp.nan,  # nan
                lambda: jnp.sum(
                    idx2species == species_keys[idx], dtype=jnp.float32
                ),  # count members
            )

        member_count = vmap(count_members)(self.species_arange)

        species_state = state.species.update(
            species_keys=species_keys,
            best_fitness=best_fitness,
            last_improved=last_improved,
            member_count=member_count,
            idx2species=idx2species,
            center_nodes=center_nodes,
            center_conns=center_conns,
            next_species_key=next_species_key,
        )

        return state.update(
            species=species_state,
        )

=== ./src/tensorneat/algorithm/neat/__init__.py ===
from .species import *
from .neat import NEAT

=== ./src/tensorneat/algorithm/base.py ===
from ..common import State, StatefulBaseClass


class BaseAlgorithm(StatefulBaseClass):
    def ask(self, state: State):
        """require the population to be evaluated"""
        raise NotImplementedError

    def tell(self, state: State, fitness):
        """update the state of the algorithm"""
        raise NotImplementedError

    def transform(self, state, individual):
        """transform the genome into a neural network"""
        raise NotImplementedError

    def forward(self, state, transformed, inputs):
        raise NotImplementedError

    def show_details(self, state: State, fitness):
        """Visualize the running details of the algorithm"""
        raise NotImplementedError

    @property
    def num_inputs(self):
        raise NotImplementedError

    @property
    def num_outputs(self):
        raise NotImplementedError

=== ./src/tensorneat/common/stateful_class.py ===
from typing import Optional
from . import State
import pickle
import datetime
import warnings


class StatefulBaseClass:
    def setup(self, state=State()):
        return state

    def show_config(self, registered_objects=None):
        if registered_objects is None:  # root call
            registered_objects = []

        config = {}
        for key, value in self.__dict__.items():
            if isinstance(value, StatefulBaseClass) and value not in registered_objects:
                registered_objects.append(value)
                config[str(key)] = value.show_config(registered_objects)

            else:
                config[str(key)] = str(value)
        return config

    # TODO: Bug need be fixed
    # def save(self, state: Optional[State] = None, path: Optional[str] = None):
    #     if path is None:
    #         time = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    #         path = f"./{self.__class__.__name__} {time}.pkl"
    #     if state is not None:
    #         self.__dict__["aux_for_state"] = state
    #     with open(path, "wb") as f:
    #         pickle.dump(self, f)

    # def __getstate__(self):
    #     # only pickle the picklable attributes
    #     state = self.__dict__.copy()
    #     non_picklable_keys = []
    #     for key, value in state.items():
    #         try:
    #             pickle.dumps(value)
    #         except Exception as e:
    #             print(f"Cannot pickle key {key}: {e}")
    #             non_picklable_keys.append(key)

    #     for key in non_picklable_keys:
    #         state.pop(key)

    #     return state

    # @classmethod
    # def load(cls, path: str, with_state: bool = False, warning: bool = True):
    #     with open(path, "rb") as f:
    #         obj = pickle.load(f)
    #     if with_state:
    #         if "aux_for_state" not in obj.__dict__:
    #             if warning:
    #                 warnings.warn(
    #                     "This object does not have state to load, return empty state",
    #                     category=UserWarning,
    #                 )
    #             return obj, State()
    #         state = obj.__dict__["aux_for_state"]
    #         del obj.__dict__["aux_for_state"]
    #         return obj, state
    #     else:
    #         if "aux_for_state" in obj.__dict__:
    #             if warning:
    #                 warnings.warn(
    #                     "This object has state to load, ignore it",
    #                     category=UserWarning,
    #                 )
    #             del obj.__dict__["aux_for_state"]
    #         return obj

=== ./src/tensorneat/common/evox_adaptors/__init__.py ===
from .algorithm_adaptor import EvoXAlgorithmAdaptor
from .tensorneat_monitor import TensorNEATMonitor

=== ./src/tensorneat/common/evox_adaptors/tensorneat_monitor.py ===
import warnings
import os
import time
import numpy as np

import jax
from jax.experimental import io_callback
from evox import Monitor
from evox import State as EvoXState

from tensorneat.algorithm import BaseAlgorithm as TensorNEATAlgorithm
from tensorneat.common import State as TensorNEATState


class TensorNEATMonitor(Monitor):

    def __init__(
        self,
        tensorneat_algorithm: TensorNEATAlgorithm,
        save_dir: str = None,
        is_save: bool = False,
    ):
        super().__init__()
        self.tensorneat_algorithm = tensorneat_algorithm

        self.generation_timestamp = time.time()
        self.alg_state: TensorNEATState = None
        self.fitness = None
        self.best_fitness = -np.inf
        self.best_genome = None

        self.is_save = is_save

        if is_save:
            if save_dir is None:
                now = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                self.save_dir = f"./{self.__class__.__name__} {now}"
            else:
                self.save_dir = save_dir
            print(f"save to {self.save_dir}")
            if not os.path.exists(self.save_dir):
                os.makedirs(self.save_dir)
            self.genome_dir = os.path.join(self.save_dir, "genomes")
            if not os.path.exists(self.genome_dir):
                os.makedirs(self.genome_dir)

    def hooks(self):
        return ["pre_tell"]

    def pre_tell(self, state: EvoXState, cand_sol, transformed_cand_sol, fitness, transformed_fitness):
        io_callback(
            self.store_info,
            None,
            state,
            transformed_fitness,
        )

    def store_info(self, state: EvoXState, fitness):
        self.alg_state: TensorNEATState = state.query_state("algorithm").alg_state
        self.fitness = jax.device_get(fitness)

    def show(self):
        pop = self.tensorneat_algorithm.ask(self.alg_state)
        generation = int(self.alg_state.generation)

        valid_fitnesses = self.fitness[~np.isinf(self.fitness)]

        max_f, min_f, mean_f, std_f = (
            max(valid_fitnesses),
            min(valid_fitnesses),
            np.mean(valid_fitnesses),
            np.std(valid_fitnesses),
        )

        new_timestamp = time.time()

        cost_time = new_timestamp - self.generation_timestamp
        self.generation_timestamp = time.time()
        
        max_idx = np.argmax(self.fitness)
        if self.fitness[max_idx] > self.best_fitness:
            self.best_fitness = self.fitness[max_idx]
            self.best_genome = pop[0][max_idx], pop[1][max_idx]

        if self.is_save:
            # save best
            best_genome = jax.device_get(self.best_genome)
            file_name = os.path.join(
                self.genome_dir, f"{generation}.npz"
            )
            with open(file_name, "wb") as f:
                np.savez(
                    f,
                    nodes=best_genome[0],
                    conns=best_genome[1],
                    fitness=self.best_fitness,
                )

            # append log
            with open(os.path.join(self.save_dir, "log.txt"), "a") as f:
                f.write(
                    f"{generation},{max_f},{min_f},{mean_f},{std_f},{cost_time}\n"
                )

        print(
            f"Generation: {generation}, Cost time: {cost_time * 1000:.2f}ms\n",
            f"\tfitness: valid cnt: {len(valid_fitnesses)}, max: {max_f:.4f}, min: {min_f:.4f}, mean: {mean_f:.4f}, std: {std_f:.4f}\n",
        )

        self.tensorneat_algorithm.show_details(self.alg_state, self.fitness)

=== ./src/tensorneat/common/evox_adaptors/algorithm_adaptor.py ===
import jax.numpy as jnp

from evox import Algorithm as EvoXAlgorithm, State as EvoXState, jit_class

from tensorneat.algorithm import BaseAlgorithm as TensorNEATAlgorithm
from tensorneat.common import State as TensorNEATState


@jit_class
class EvoXAlgorithmAdaptor(EvoXAlgorithm):
    def __init__(self, algorithm: TensorNEATAlgorithm):
        self.algorithm = algorithm
        self.fixed_state = None

    def setup(self, key):
        neat_algorithm_state = TensorNEATState(randkey=key)
        neat_algorithm_state = self.algorithm.setup(neat_algorithm_state)
        self.fixed_state = neat_algorithm_state
        return EvoXState(alg_state=neat_algorithm_state)

    def ask(self, state: EvoXState):
        population = self.algorithm.ask(state.alg_state)
        return population, state

    def tell(self, state: EvoXState, fitness):
        fitness = jnp.where(jnp.isnan(fitness), -jnp.inf, fitness)
        neat_algorithm_state = self.algorithm.tell(state.alg_state, fitness)
        return state.replace(alg_state=neat_algorithm_state)

    def transform(self, individual):
        return self.algorithm.transform(self.fixed_state, individual)

    def forward(self, transformed, inputs):
        return self.algorithm.forward(self.fixed_state, transformed, inputs)

=== ./src/tensorneat/common/sympy_tools.py ===
import re
import sympy as sp

def analysis_nodes_exprs(nodes_exprs):
    input_cnt, hidden_cnt, output_cnt = 0, 0, 0
    norm_symbols = {}
    for key in nodes_exprs.keys():
        if str(key).startswith('i'):
            input_cnt += 1
        elif str(key).startswith('h'):
            hidden_cnt += 1
        elif str(key).startswith('o'):
            output_cnt += 1
        elif str(key).startswith('norm'):
            norm_symbols[key] = nodes_exprs[key]
    return input_cnt, hidden_cnt, output_cnt, norm_symbols

def round_expr(expr, precision=2):
    """
    Round numerical values in a sympy expression to a given precision.
    """
    return expr.xreplace({n: round(n, precision) for n in expr.atoms(sp.Number)})


def replace_variable_names(expression):
    """
    Transform sympy expression to a string with array index that can be used in python code.
    For example, `o0` will be transformed to `o[0]`.
    """
    expression_str = str(expression)
    expression_str = re.sub(r"\bo(\d+)\b", r"o[\1]", expression_str)
    expression_str = re.sub(r"\bh(\d+)\b", r"h[\1]", expression_str)
    expression_str = re.sub(r"\bi(\d+)\b", r"i[\1]", expression_str)
    return expression_str


def to_latex_code(symbols, args_symbols, input_symbols, nodes_exprs, output_exprs, use_hidden_nodes=True):
    input_cnt, hidden_cnt, output_cnt, norm_symbols = analysis_nodes_exprs(nodes_exprs)
    res = "\\begin{align}\n"
    
    if not use_hidden_nodes:
        for i in range(output_cnt):
            expr = output_exprs[i].subs(args_symbols)
            rounded_expr = round_expr(expr, 2)
            latex_expr = f"o_{{{sp.latex(i)}}} &= {sp.latex(rounded_expr)}\\newline\n"
            res += latex_expr
    else:
        for i in range(hidden_cnt):
            symbol = sp.symbols(f"h{i}")
            expr = nodes_exprs[symbol].subs(args_symbols).subs(norm_symbols)
            rounded_expr = round_expr(expr, 2)
            latex_expr = f"h_{{{sp.latex(i)}}} &= {sp.latex(rounded_expr)}\\newline\n"
            res += latex_expr
        for i in range(output_cnt):
            symbol = sp.symbols(f"o{i}")
            expr = nodes_exprs[symbol].subs(args_symbols).subs(norm_symbols)
            rounded_expr = round_expr(expr, 2)
            latex_expr = f"o_{{{sp.latex(i)}}} &= {sp.latex(rounded_expr)}\\newline\n"
            res += latex_expr
    res += "\\end{align}\n"
    return res


def to_python_code(symbols, args_symbols, input_symbols, nodes_exprs, output_exprs, use_hidden_nodes=True):
    input_cnt, hidden_cnt, output_cnt, norm_symbols = analysis_nodes_exprs(nodes_exprs)
    res = ""
    if not use_hidden_nodes:
        # pre-allocate space
        res += f"o = np.zeros({output_cnt})\n"
        for i in range(output_cnt):
            expr = output_exprs[i].subs(args_symbols)
            rounded_expr = round_expr(expr, 6)
            str_expr = f"o{i} = {rounded_expr}"
            res += replace_variable_names(str_expr) + "\n"
    else:
        # pre-allocate space
        res += f"h = np.zeros({hidden_cnt})\n"
        res += f"o = np.zeros({output_cnt})\n"
        for i in range(hidden_cnt):
            symbol = sp.symbols(f"h{i}")
            expr = nodes_exprs[symbol].subs(args_symbols).subs(norm_symbols)
            rounded_expr = round_expr(expr, 6)
            str_expr = f"h{i} = {rounded_expr}"
            res += replace_variable_names(str_expr) + "\n"
        for i in range(output_cnt):
            symbol = sp.symbols(f"o{i}")
            expr = nodes_exprs[symbol].subs(args_symbols).subs(norm_symbols)
            rounded_expr = round_expr(expr, 6)
            str_expr = f"o{i} = {rounded_expr}"
            res += replace_variable_names(str_expr) + "\n"
    return res
=== ./src/tensorneat/common/tools.py ===
from functools import partial

import numpy as np
import jax
from jax import numpy as jnp, Array, jit, vmap

# infinite int, use to represent the unavialable index in int32 array.
# as we can not use nan in int32 array
I_INF = np.iinfo(jnp.int32).max  


def attach_with_inf(arr, idx):
    target_dim = arr.ndim + idx.ndim - 1
    expand_idx = jnp.expand_dims(idx, axis=tuple(range(idx.ndim, target_dim)))

    return jnp.where(expand_idx == I_INF, jnp.nan, arr[idx])


@jit
def fetch_first(mask, default=I_INF) -> Array:
    """
    fetch the first True index
    :param mask: array of bool
    :param default: the default value if no element satisfying the condition
    :return: the index of the first element satisfying the condition. if no element satisfying the condition, return default value
    """
    idx = jnp.argmax(mask)
    return jnp.where(mask[idx], idx, default)


@jit
def fetch_random(randkey, mask, default=I_INF) -> Array:
    """
    similar to fetch_first, but fetch a random True index
    """
    true_cnt = jnp.sum(mask)
    cumsum = jnp.cumsum(mask)
    target = jax.random.randint(randkey, shape=(), minval=1, maxval=true_cnt + 1)
    mask = jnp.where(true_cnt == 0, False, cumsum >= target)
    return fetch_first(mask, default)


@partial(jit, static_argnames=["reverse"])
def rank_elements(array, reverse=False):
    """
    rank the element in the array.
    if reverse is True, the rank is from small to large. default large to small
    """
    if not reverse:
        array = -array
    return jnp.argsort(jnp.argsort(array))


@jit
def mutate_float(
    randkey, val, init_mean, init_std, mutate_power, mutate_rate, replace_rate
):
    """
    mutate a float value
    uniformly pick r from [0, 1]
    r in [0, mutate_rate) -> add noise
    r in [mutate_rate, mutate_rate + replace_rate) -> create a new value to replace the original value
    otherwise -> keep the original value
    """
    k1, k2, k3 = jax.random.split(randkey, num=3)
    noise = jax.random.normal(k1, ()) * mutate_power
    replace = jax.random.normal(k2, ()) * init_std + init_mean
    r = jax.random.uniform(k3, ())

    val = jnp.where(
        r < mutate_rate,
        val + noise,
        jnp.where((mutate_rate < r) & (r < mutate_rate + replace_rate), replace, val),
    )

    return val


@jit
def mutate_int(randkey, val, options, replace_rate):
    """
    mutate an int value
    uniformly pick r from [0, 1]
    r in [0, replace_rate) -> create a new value to replace the original value
    otherwise -> keep the original value
    """
    k1, k2 = jax.random.split(randkey, num=2)
    r = jax.random.uniform(k1, ())

    val = jnp.where(r < replace_rate, jax.random.choice(k2, options), val)

    return val


def argmin_with_mask(arr, mask):
    """
    find the index of the minimum element in the array, but only consider the element with True mask
    """
    masked_arr = jnp.where(mask, arr, jnp.inf)
    min_idx = jnp.argmin(masked_arr)
    return min_idx


def hash_array(arr: Array):
    """
    Hash an array of uint32 to a single uint
    """
    arr = jax.lax.bitcast_convert_type(arr, jnp.uint32)

    def update(i, hash_val):
        return hash_val ^ (
            arr[i] + jnp.uint32(0x9E3779B9) + (hash_val << 6) + (hash_val >> 2)
        )

    return jax.lax.fori_loop(0, arr.size, update, jnp.uint32(0))

=== ./src/tensorneat/common/graph.py ===
"""
Some graph algorithm implemented in jax and python.
"""

import jax
from jax import jit, Array, numpy as jnp
from typing import Tuple, Set, List, Union

from .tools import fetch_first, I_INF


@jit
def topological_sort(nodes: Array, conns: Array) -> Array:
    """
    a jit-able version of topological_sort!
    conns: Array[N, N]
    """

    in_degree = jnp.where(jnp.isnan(nodes[:, 0]), jnp.nan, jnp.sum(conns, axis=0))
    res = jnp.full(in_degree.shape, I_INF)

    def cond_fun(carry):
        res_, idx_, in_degree_ = carry
        i = fetch_first(in_degree_ == 0.0)
        return i != I_INF

    def body_func(carry):
        res_, idx_, in_degree_ = carry
        i = fetch_first(in_degree_ == 0.0)

        # add to res and flag it is already in it
        res_ = res_.at[idx_].set(i)
        in_degree_ = in_degree_.at[i].set(-1)

        # decrease in_degree of all its children
        children = conns[i, :]
        in_degree_ = jnp.where(children, in_degree_ - 1, in_degree_)
        return res_, idx_ + 1, in_degree_

    res, _, _ = jax.lax.while_loop(cond_fun, body_func, (res, 0, in_degree))
    return res


def topological_sort_python(
    nodes: Union[Set[int], List[int]],
    conns: Union[Set[Tuple[int, int]], List[Tuple[int, int]]],
) -> Tuple[List[int], List[List[int]]]:
    # a python version of topological_sort, use python set to store nodes and conns
    # returns the topological order of the nodes and the topological layers
    # written by gpt4 :)

    # Make a copy of the input nodes and connections
    nodes = nodes.copy()
    conns = conns.copy()

    # Initialize the in-degree of each node to 0
    in_degree = {node: 0 for node in nodes}

    # Compute the in-degree for each node
    for conn in conns:
        in_degree[conn[1]] += 1

    topo_order = []
    topo_layer = []

    # Find all nodes with in-degree 0
    zero_in_degree_nodes = [node for node in nodes if in_degree[node] == 0]

    while zero_in_degree_nodes:

        for node in zero_in_degree_nodes:
            nodes.remove(node)

        zero_in_degree_nodes = sorted(
            zero_in_degree_nodes
        )  # make sure the topo_order is from small to large

        topo_layer.append(zero_in_degree_nodes.copy())

        for node in zero_in_degree_nodes:
            topo_order.append(node)

            # Iterate over all connections and reduce the in-degree of connected nodes
            for conn in list(conns):
                if conn[0] == node:
                    in_degree[conn[1]] -= 1
                    conns.remove(conn)

        zero_in_degree_nodes = [node for node in nodes if in_degree[node] == 0]

    # Check if there are still connections left indicating a cycle
    if conns or nodes:
        raise ValueError("Graph has at least one cycle, topological sort not possible")

    return topo_order, topo_layer


@jit
def check_cycles(nodes: Array, conns: Array, from_idx, to_idx) -> Array:
    """
    Check whether a new connection (from_idx -> to_idx) will cause a cycle.
    """

    conns = conns.at[from_idx, to_idx].set(True)

    visited = jnp.full(nodes.shape[0], False)
    new_visited = visited.at[to_idx].set(True)

    def cond_func(carry):
        visited_, new_visited_ = carry
        end_cond1 = jnp.all(visited_ == new_visited_)  # no new nodes been visited
        end_cond2 = new_visited_[from_idx]  # the starting node has been visited
        return jnp.logical_not(end_cond1 | end_cond2)

    def body_func(carry):
        _, visited_ = carry
        new_visited_ = jnp.dot(visited_, conns)
        new_visited_ = jnp.logical_or(visited_, new_visited_)
        return visited_, new_visited_

    _, visited = jax.lax.while_loop(cond_func, body_func, (visited, new_visited))
    return visited[from_idx]

=== ./src/tensorneat/common/__init__.py ===
from .tools import *
from .graph import *
from .state import State
from .stateful_class import StatefulBaseClass

from .functions import ACT, AGG, apply_activation, apply_aggregation, get_func_name

=== ./src/tensorneat/common/functions/agg_sympy.py ===
import numpy as np
import sympy as sp


class SympySum(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Add(*z)


class SympyProduct(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Mul(*z)


class SympyMax(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Max(*z)


class SympyMin(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Min(*z)


class SympyMaxabs(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Max(*z, key=sp.Abs)


class SympyMean(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Add(*z) / len(z)

=== ./src/tensorneat/common/functions/__init__.py ===
import jax, jax.numpy as jnp

from .act_jnp import *
from .act_sympy import *
from .agg_jnp import *
from .agg_sympy import *
from .manager import FunctionManager

act_name2jnp = {
    "scaled_sigmoid": scaled_sigmoid_,
    "sigmoid": sigmoid_,
    "scaled_tanh": scaled_tanh_,
    "tanh": tanh_,
    "sin": sin_,
    "relu": relu_,
    "lelu": lelu_,
    "identity": identity_,
    "inv": inv_,
    "log": log_,
    "exp": exp_,
    "abs": abs_,
}

act_name2sympy = {
    "scaled_sigmoid": SympyScaledSigmoid,
    "sigmoid": SympySigmoid,
    "scaled_tanh": SympyScaledTanh,
    "tanh": SympyTanh,
    "sin": SympySin,
    "relu": SympyRelu,
    "lelu": SympyLelu,
    "identity": SympyIdentity,
    "inv": SympyIdentity,
    "log": SympyLog,
    "exp": SympyExp,
    "abs": SympyAbs,
    "clip": SympyClip,
}

agg_name2jnp = {
    "sum": sum_,
    "product": product_,
    "max": max_,
    "min": min_,
    "maxabs": maxabs_,
    "mean": mean_,
}

agg_name2sympy = {
    "sum": SympySum,
    "product": SympyProduct,
    "max": SympyMax,
    "min": SympyMin,
    "maxabs": SympyMaxabs,
    "mean": SympyMean,
}

ACT = FunctionManager(act_name2jnp, act_name2sympy)
AGG = FunctionManager(agg_name2jnp, agg_name2sympy)

def apply_activation(idx, z, act_funcs):
    """
    calculate activation function for each node
    """
    idx = jnp.asarray(idx, dtype=jnp.int32)
    # change idx from float to int

    # -1 means identity activation
    res = jax.lax.cond(
        idx == -1,
        lambda: z,
        lambda: jax.lax.switch(idx, act_funcs, z),
    )

    return res

def apply_aggregation(idx, z, agg_funcs):
    """
    calculate activation function for inputs of node
    """
    idx = jnp.asarray(idx, dtype=jnp.int32)

    return jax.lax.cond(
        jnp.all(jnp.isnan(z)),
        lambda: jnp.nan,  # all inputs are nan
        lambda: jax.lax.switch(idx, agg_funcs, z),  # otherwise
    )

def get_func_name(func):
    name = func.__name__
    if name.endswith("_"):
        name = name[:-1]
    return name
=== ./src/tensorneat/common/functions/act_jnp.py ===
import jax.numpy as jnp

SCALE = 3


def scaled_sigmoid_(z):
    z = 1 / (1 + jnp.exp(-z))
    return z * SCALE


def sigmoid_(z):
    z = 1 / (1 + jnp.exp(-z))
    return z


def scaled_tanh_(z):
    return jnp.tanh(z) * SCALE


def tanh_(z):
    return jnp.tanh(z)


def sin_(z):
    return jnp.sin(z)


def relu_(z):
    return jnp.maximum(z, 0)


def lelu_(z):
    leaky = 0.005
    return jnp.where(z > 0, z, leaky * z)


def identity_(z):
    return z


def inv_(z):
    # avoid division by zero
    z = jnp.where(z > 0, jnp.maximum(z, 1e-7), jnp.minimum(z, -1e-7))
    return 1 / z


def log_(z):
    z = jnp.maximum(z, 1e-7)
    return jnp.log(z)


def exp_(z):
    return jnp.exp(z)


def abs_(z):
    return jnp.abs(z)

=== ./src/tensorneat/common/functions/agg_jnp.py ===
import jax.numpy as jnp


def sum_(z):
    return jnp.sum(z, axis=0, where=~jnp.isnan(z), initial=0)


def product_(z):
    return jnp.prod(z, axis=0, where=~jnp.isnan(z), initial=1)


def max_(z):
    return jnp.max(z, axis=0, where=~jnp.isnan(z), initial=-jnp.inf)


def min_(z):
    return jnp.min(z, axis=0, where=~jnp.isnan(z), initial=jnp.inf)


def maxabs_(z):
    z = jnp.where(jnp.isnan(z), 0, z)
    abs_z = jnp.abs(z)
    max_abs_index = jnp.argmax(abs_z)
    return z[max_abs_index]

def mean_(z):
    sumation = sum_(z)
    valid_count = jnp.sum(~jnp.isnan(z), axis=0)
    return sumation / valid_count

=== ./src/tensorneat/common/functions/act_sympy.py ===
import sympy as sp
import numpy as np

SCALE = 3

class SympySigmoid(sp.Function):
    @classmethod
    def eval(cls, z):
        z = 1 / (1 + sp.exp(-z))
        return z


class SympyScaledSigmoid(sp.Function):
    @classmethod
    def eval(cls, z):
        return SympySigmoid(z) * SCALE


class SympyTanh(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.tanh(z)


class SympyScaledTanh(sp.Function):
    @classmethod
    def eval(cls, z):
        return SympyTanh(z) * SCALE


class SympySin(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.sin(z)


class SympyRelu(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Max(z, 0)


class SympyLelu(sp.Function):
    @classmethod
    def eval(cls, z):
        leaky = 0.005
        return sp.Piecewise((z, z > 0), (leaky * z, True))


class SympyIdentity(sp.Function):
    @classmethod
    def eval(cls, z):
        return z


class SympyInv(sp.Function):
    @classmethod
    def eval(cls, z):
        z = sp.Piecewise((sp.Max(z, 1e-7), z > 0), (sp.Min(z, -1e-7), True))
        return 1 / z


class SympyLog(sp.Function):
    @classmethod
    def eval(cls, z):
        z = sp.Max(z, 1e-7)
        return sp.log(z)


class SympyExp(sp.Function):
    @classmethod
    def eval(cls, z):
        z = SympyClip(z, -10, 10)
        return sp.exp(z)


class SympyAbs(sp.Function):
    @classmethod
    def eval(cls, z):
        return sp.Abs(z)


class SympyClip(sp.Function):
    @classmethod
    def eval(cls, val, min_val, max_val):
        if val.is_Number and min_val.is_Number and max_val.is_Number:
            return sp.Piecewise(
                (min_val, val < min_val), (max_val, val > max_val), (val, True)
            )
        return None

    @staticmethod
    def numerical_eval(val, min_val, max_val, backend=np):
        return backend.clip(val, min_val, max_val)

    def _sympystr(self, printer):
        return f"clip({self.args[0]}, {self.args[1]}, {self.args[2]})"

    def _latex(self, printer):
        return rf"\mathrm{{clip}}\left({sp.latex(self.args[0])}, {self.args[1]}, {self.args[2]}\right)"

=== ./src/tensorneat/common/functions/manager.py ===
from functools import partial
import numpy as np
import jax.numpy as jnp
from typing import Union, Callable
import sympy as sp


class FunctionManager:

    def __init__(self, name2jnp, name2sympy):
        self.name2jnp = name2jnp
        self.name2sympy = name2sympy
        for name, func in name2jnp.items():
            setattr(self, name, func)

    def get_all_funcs(self):
        all_funcs = []
        for name in self.name2jnp:
            all_funcs.append(getattr(self, name))
        return all_funcs

    def add_func(self, name, func):
        if not callable(func):
            raise ValueError("The provided function is not callable")
        if name in self.name2jnp:
            raise ValueError(f"The provided name={name} is already in use")

        self.name2jnp[name] = func
        setattr(self, name, func)

    def update_sympy(self, name, sympy_cls: sp.Function):
        self.name2sympy[name] = sympy_cls

    def obtain_sympy(self, func: Union[str, Callable]):
        if isinstance(func, str):
            if func not in self.name2sympy:
                raise ValueError(f"Func {func} doesn't have a sympy representation.")
            return self.name2sympy[func]

        elif isinstance(func, Callable):
            # try to find name
            for name, f in self.name2jnp.items():
                if f == func:
                    return self._obtain_sympy_by_name(name)
            raise ValueError(f"Func {func} doesn't not registered.")

        else:
            raise ValueError(f"Func {func} need be a string or callable.")

    def _obtain_sympy_by_name(self, name: str):
        if name not in self.name2sympy:
            raise ValueError(f"Func {name} doesn't have a sympy representation.")
        return self.name2sympy[name]

    def sympy_module(self, backend: str):
        assert backend in ["jax", "numpy"]
        if backend == "jax":
            backend = jnp
        elif backend == "numpy":
            backend = np
        module = {}
        for sympy_cls in self.name2sympy.values():
            if hasattr(sympy_cls, "numerical_eval"):
                module[sympy_cls.__name__] = partial(sympy_cls.numerical_eval, backend)

        return module

=== ./src/tensorneat/common/state.py ===
import pickle

from jax.tree_util import register_pytree_node_class


@register_pytree_node_class
class State:
    def __init__(self, **kwargs):
        self.__dict__["state_dict"] = kwargs

    def registered_keys(self):
        return self.state_dict.keys()

    def register(self, **kwargs):
        for key in kwargs:
            if key in self.registered_keys():
                raise ValueError(f"Key {key} already exists in state")
        return State(**{**self.state_dict, **kwargs})

    def update(self, **kwargs):
        for key in kwargs:
            if key not in self.registered_keys():
                raise ValueError(f"Key {key} does not exist in state")
        return State(**{**self.state_dict, **kwargs})

    def __getattr__(self, name):
        return self.state_dict[name]

    def __setattr__(self, name, value):
        raise AttributeError("State is immutable")

    def __repr__(self):
        return f"State ({self.state_dict})"

    def __getstate__(self):
        return self.state_dict.copy()

    def __setstate__(self, state):
        self.__dict__["state_dict"] = state

    def __contains__(self, item):
        return item in self.state_dict

    def save(self, file_name):
        with open(file_name, "wb") as f:
            pickle.dump(self, f)

    @classmethod
    def load(cls, file_name):
        with open(file_name, "rb") as f:
            return pickle.load(f)

    def tree_flatten(self):
        children = list(self.state_dict.values())
        aux_data = list(self.state_dict.keys())
        return children, aux_data

    @classmethod
    def tree_unflatten(cls, aux_data, children):
        return cls(**dict(zip(aux_data, children)))

=== ./src/tensorneat/pipeline.py ===
import json
import os

import jax, jax.numpy as jnp
import datetime, time
import numpy as np

from tensorneat.algorithm import BaseAlgorithm
from tensorneat.problem import BaseProblem
from tensorneat.common import State, StatefulBaseClass


class Pipeline(StatefulBaseClass):
    def __init__(
        self,
        algorithm: BaseAlgorithm,
        problem: BaseProblem,
        seed: int = 42,
        fitness_target: float = 1,
        generation_limit: int = 1000,
        is_save: bool = False,
        save_dir=None,
    ):
        assert problem.jitable, "Currently, problem must be jitable"

        self.algorithm = algorithm
        self.problem = problem
        self.seed = seed
        self.fitness_target = fitness_target
        self.generation_limit = generation_limit
        self.pop_size = self.algorithm.pop_size

        np.random.seed(self.seed)

        assert (
            algorithm.num_inputs == self.problem.input_shape[-1]
        ), f"algorithm input shape is {algorithm.num_inputs} but problem input shape is {self.problem.input_shape}"

        self.best_genome = None
        self.best_fitness = float("-inf")
        self.generation_timestamp = None
        self.is_save = is_save

        if is_save:
            if save_dir is None:
                now = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
                self.save_dir = f"./{self.__class__.__name__} {now}"
            else:
                self.save_dir = save_dir
            print(f"save to {self.save_dir}")
            if not os.path.exists(self.save_dir):
                os.makedirs(self.save_dir)
            self.genome_dir = os.path.join(self.save_dir, "genomes")
            if not os.path.exists(self.genome_dir):
                os.makedirs(self.genome_dir)

    def setup(self, state=State()):
        print("initializing")
        state = state.register(randkey=jax.random.PRNGKey(self.seed))

        state = self.algorithm.setup(state)
        state = self.problem.setup(state)

        if self.is_save:
            # self.save(state=state, path=os.path.join(self.save_dir, "pipeline.pkl"))
            with open(os.path.join(self.save_dir, "config.txt"), "w") as f:
                f.write(json.dumps(self.show_config(), indent=4))
            # create log file
            with open(os.path.join(self.save_dir, "log.txt"), "w") as f:
                f.write("Generation,Max,Min,Mean,Std,Cost Time\n")

        print("initializing finished")
        return state

    def step(self, state):

        randkey_, randkey = jax.random.split(state.randkey)
        keys = jax.random.split(randkey_, self.pop_size)

        pop = self.algorithm.ask(state)

        pop_transformed = jax.vmap(self.algorithm.transform, in_axes=(None, 0))(
            state, pop
        )

        fitnesses = jax.vmap(self.problem.evaluate, in_axes=(None, 0, None, 0))(
            state, keys, self.algorithm.forward, pop_transformed
        )

        # replace nan with -inf
        fitnesses = jnp.where(jnp.isnan(fitnesses), -jnp.inf, fitnesses)

        previous_pop = self.algorithm.ask(state)
        state = self.algorithm.tell(state, fitnesses)

        return state.update(randkey=randkey), previous_pop, fitnesses

    def auto_run(self, state):
        print("start compile")
        tic = time.time()
        compiled_step = jax.jit(self.step).lower(state).compile()
        # compiled_step = self.step
        print(
            f"compile finished, cost time: {time.time() - tic:.6f}s",
        )

        for _ in range(self.generation_limit):

            self.generation_timestamp = time.time()

            state, previous_pop, fitnesses = compiled_step(state)

            fitnesses = jax.device_get(fitnesses)

            self.analysis(state, previous_pop, fitnesses)

            if max(fitnesses) >= self.fitness_target:
                print("Fitness limit reached!")
                break

        if int(state.generation) >= self.generation_limit:
            print("Generation limit reached!")

        if self.is_save:
            best_genome = jax.device_get(self.best_genome)
            with open(os.path.join(self.genome_dir, f"best_genome.npz"), "wb") as f:
                np.savez(
                    f,
                    nodes=best_genome[0],
                    conns=best_genome[1],
                    fitness=self.best_fitness,
                )

        return state, self.best_genome

    def analysis(self, state, pop, fitnesses):
        
        generation = int(state.generation)

        valid_fitnesses = fitnesses[~np.isinf(fitnesses)]

        max_f, min_f, mean_f, std_f = (
            max(valid_fitnesses),
            min(valid_fitnesses),
            np.mean(valid_fitnesses),
            np.std(valid_fitnesses),
        )

        new_timestamp = time.time()

        cost_time = new_timestamp - self.generation_timestamp

        max_idx = np.argmax(fitnesses)
        if fitnesses[max_idx] > self.best_fitness:
            self.best_fitness = fitnesses[max_idx]
            self.best_genome = pop[0][max_idx], pop[1][max_idx]

        if self.is_save:
            # save best
            best_genome = jax.device_get((pop[0][max_idx], pop[1][max_idx]))
            file_name = os.path.join(
                self.genome_dir, f"{generation}.npz"
            )
            with open(file_name, "wb") as f:
                np.savez(
                    f,
                    nodes=best_genome[0],
                    conns=best_genome[1],
                    fitness=self.best_fitness,
                )

            # append log
            with open(os.path.join(self.save_dir, "log.txt"), "a") as f:
                f.write(
                    f"{generation},{max_f},{min_f},{mean_f},{std_f},{cost_time}\n"
                )

        print(
            f"Generation: {generation}, Cost time: {cost_time * 1000:.2f}ms\n",
            f"\tfitness: valid cnt: {len(valid_fitnesses)}, max: {max_f:.4f}, min: {min_f:.4f}, mean: {mean_f:.4f}, std: {std_f:.4f}\n",
        )

        self.algorithm.show_details(state, fitnesses)

    def show(self, state, best, *args, **kwargs):
        transformed = self.algorithm.transform(state, best)
        return self.problem.show(
            state, state.randkey, self.algorithm.forward, transformed, *args, **kwargs
        )

=== ./src/tensorneat/genome/recurrent.py ===
import jax
from jax import vmap, numpy as jnp
from .utils import unflatten_conns

from .base import BaseGenome
from .gene import DefaultNode, DefaultConn
from .operations import DefaultMutation, DefaultCrossover, DefaultDistance
from .utils import unflatten_conns, extract_gene_attrs, extract_gene_attrs

from ..common import attach_with_inf

class RecurrentGenome(BaseGenome):
    """Default genome class, with the same behavior as the NEAT-Python"""

    network_type = "recurrent"

    def __init__(
        self,
        num_inputs: int,
        num_outputs: int,
        max_nodes=50,
        max_conns=100,
        node_gene=DefaultNode(),
        conn_gene=DefaultConn(),
        mutation=DefaultMutation(),
        crossover=DefaultCrossover(),
        distance=DefaultDistance(),
        output_transform=None,
        input_transform=None,
        init_hidden_layers=(),
        activate_time=10,
    ):
        super().__init__(
            num_inputs,
            num_outputs,
            max_nodes,
            max_conns,
            node_gene,
            conn_gene,
            mutation,
            crossover,
            distance,
            output_transform,
            input_transform,
            init_hidden_layers,
        )
        self.activate_time = activate_time

    def transform(self, state, nodes, conns):
        u_conns = unflatten_conns(nodes, conns)
        return nodes, conns, u_conns

    def forward(self, state, transformed, inputs):
        nodes, conns, u_conns = transformed

        vals = jnp.full((self.max_nodes,), jnp.nan)

        nodes_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(self.node_gene, nodes)
        conns_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(self.conn_gene, conns)
        expand_conns_attrs = attach_with_inf(conns_attrs, u_conns)

        def body_func(_, values):

            # set input values
            values = values.at[self.input_idx].set(inputs)

            # calculate connections
            node_ins = vmap(
                vmap(self.conn_gene.forward, in_axes=(None, 0, None)),
                in_axes=(None, 0, 0),
            )(state, expand_conns_attrs, values)

            # calculate nodes
            is_output_nodes = jnp.isin(nodes[:, 0], self.output_idx)
            values = vmap(self.node_gene.forward, in_axes=(None, 0, 0, 0))(
                state, nodes_attrs, node_ins.T, is_output_nodes
            )

            return values

        vals = jax.lax.fori_loop(0, self.activate_time, body_func, vals)

        if self.output_transform is None:
            return vals[self.output_idx]
        else:
            return self.output_transform(vals[self.output_idx])

    def sympy_func(self, state, network, precision=3):
        raise ValueError("Sympy function is not supported for Recurrent Network!")

    def visualize(self, network):
        raise ValueError("Visualize function is not supported for Recurrent Network!")

=== ./src/tensorneat/genome/__init__.py ===
from .gene import *
from .operations import *
from .base import BaseGenome
from .default import DefaultGenome
from .recurrent import RecurrentGenome


=== ./src/tensorneat/genome/gene/__init__.py ===
from .base import BaseGene
from .conn import *
from .node import *

=== ./src/tensorneat/genome/gene/node/bias.py ===
from typing import Union, Sequence, Callable, Optional

import numpy as np
import jax, jax.numpy as jnp
import sympy as sp
from ....common import (
    ACT,
    AGG,
    apply_activation,
    apply_aggregation,
    mutate_int,
    mutate_float,
    get_func_name,
)

from .base import BaseNode


class BiasNode(BaseNode):
    """
    Default node gene, with the same behavior as in NEAT-python.
    The attribute response is removed.
    """

    custom_attrs = ["bias", "aggregation", "activation"]

    def __init__(
        self,
        bias_init_mean: float = 0.0,
        bias_init_std: float = 1.0,
        bias_mutate_power: float = 0.15,
        bias_mutate_rate: float = 0.2,
        bias_replace_rate: float = 0.015,
        bias_lower_bound: float = -5,
        bias_upper_bound: float = 5,
        aggregation_default: Optional[Callable] = None,
        aggregation_options: Union[Callable, Sequence[Callable]] = AGG.sum,
        aggregation_replace_rate: float = 0.1,
        activation_default: Optional[Callable] = None,
        activation_options: Union[Callable, Sequence[Callable]] = ACT.sigmoid,
        activation_replace_rate: float = 0.1,
    ):
        super().__init__()

        if isinstance(aggregation_options, Callable):
            aggregation_options = [aggregation_options]
        if isinstance(activation_options, Callable):
            activation_options = [activation_options]

        if aggregation_default is None:
            aggregation_default = aggregation_options[0]
        if activation_default is None:
            activation_default = activation_options[0]

        self.bias_init_mean = bias_init_mean
        self.bias_init_std = bias_init_std
        self.bias_mutate_power = bias_mutate_power
        self.bias_mutate_rate = bias_mutate_rate
        self.bias_replace_rate = bias_replace_rate
        self.bias_lower_bound = bias_lower_bound
        self.bias_upper_bound = bias_upper_bound

        self.aggregation_default = aggregation_options.index(aggregation_default)
        self.aggregation_options = aggregation_options
        self.aggregation_indices = np.arange(len(aggregation_options))
        self.aggregation_replace_rate = aggregation_replace_rate

        self.activation_default = activation_options.index(activation_default)
        self.activation_options = activation_options
        self.activation_indices = np.arange(len(activation_options))
        self.activation_replace_rate = activation_replace_rate

    def new_identity_attrs(self, state):
        return jnp.array(
            [0, self.aggregation_default, -1]
        )  # activation=-1 means ACT.identity

    def new_random_attrs(self, state, randkey):
        k1, k2, k3 = jax.random.split(randkey, num=3)
        bias = jax.random.normal(k1, ()) * self.bias_init_std + self.bias_init_mean
        bias = jnp.clip(bias, self.bias_lower_bound, self.bias_upper_bound)
        agg = jax.random.choice(k2, self.aggregation_indices)
        act = jax.random.choice(k3, self.activation_indices)

        return jnp.array([bias, agg, act])

    def mutate(self, state, randkey, attrs):
        k1, k2, k3 = jax.random.split(randkey, num=3)
        bias, agg, act = attrs

        bias = mutate_float(
            k1,
            bias,
            self.bias_init_mean,
            self.bias_init_std,
            self.bias_mutate_power,
            self.bias_mutate_rate,
            self.bias_replace_rate,
        )
        bias = jnp.clip(bias, self.bias_lower_bound, self.bias_upper_bound)
        agg = mutate_int(
            k2, agg, self.aggregation_indices, self.aggregation_replace_rate
        )

        act = mutate_int(k3, act, self.activation_indices, self.activation_replace_rate)

        return jnp.array([bias, agg, act])

    def distance(self, state, attrs1, attrs2):
        bias1, agg1, act1 = attrs1
        bias2, agg2, act2 = attrs2

        return jnp.abs(bias1 - bias2) + (agg1 != agg2) + (act1 != act2)

    def forward(self, state, attrs, inputs, is_output_node=False):
        bias, agg, act = attrs

        z = apply_aggregation(agg, inputs, self.aggregation_options)
        z = bias + z

        # the last output node should not be activated
        z = jax.lax.cond(
            is_output_node,
            lambda: z,
            lambda: apply_activation(act, z, self.activation_options),
        )

        return z

    def repr(self, state, node, precision=2, idx_width=3, func_width=8):
        idx, bias, agg, act = node

        idx = int(idx)
        bias = round(float(bias), precision)
        agg = int(agg)
        act = int(act)

        if act == -1:
            act_func = ACT.identity
        else:
            act_func = self.activation_options[act]
        return "{}(idx={:<{idx_width}}, bias={:<{float_width}}, aggregation={:<{func_width}}, activation={:<{func_width}})".format(
            self.__class__.__name__,
            idx,
            bias,
            get_func_name(self.aggregation_options[agg]),
            get_func_name(act_func),
            idx_width=idx_width,
            float_width=precision + 3,
            func_width=func_width,
        )

    def to_dict(self, state, node):
        idx, bias, agg, act = node

        idx = int(idx)

        bias = jnp.float32(bias)
        agg = int(agg)
        act = int(act)

        if act == -1:
            act_func = ACT.identity
        else:
            act_func = self.activation_options[act]

        return {
            "idx": idx,
            "bias": bias,
            "agg": get_func_name(self.aggregation_options[agg]),
            "act": get_func_name(act_func),
        }

    def sympy_func(self, state, node_dict, inputs, is_output_node=False):
        bias = sp.symbols(f"n_{node_dict['idx']}_b")

        z = AGG.obtain_sympy(node_dict["agg"])(inputs)

        z = bias + z
        if is_output_node:
            pass
        else:
            z = ACT.obtain_sympy(node_dict["act"])(z)

        return z, {bias: node_dict["bias"]}

=== ./src/tensorneat/genome/gene/node/__init__.py ===
from .base import BaseNode
from .default import DefaultNode
from .bias import BiasNode
from .origin import OriginNode

=== ./src/tensorneat/genome/gene/node/default.py ===
from typing import Optional, Union, Sequence, Callable

import numpy as np
import jax, jax.numpy as jnp
import sympy as sp

from ....common import (
    ACT,
    AGG,
    apply_activation,
    apply_aggregation,
    mutate_int,
    mutate_float,
    get_func_name,
)

from .base import BaseNode


class DefaultNode(BaseNode):
    "Default node gene, with the same behavior as in NEAT-python."

    custom_attrs = ["bias", "response", "aggregation", "activation"]

    def __init__(
        self,
        bias_init_mean: float = 0.0,
        bias_init_std: float = 1.0,
        bias_mutate_power: float = 0.15,
        bias_mutate_rate: float = 0.2,
        bias_replace_rate: float = 0.015,
        bias_lower_bound: float = -5,
        bias_upper_bound: float = 5,
        response_init_mean: float = 1.0,
        response_init_std: float = 0.0,
        response_mutate_power: float = 0.15,
        response_mutate_rate: float = 0.2,
        response_replace_rate: float = 0.015,
        response_lower_bound: float = -5,
        response_upper_bound: float = 5,
        aggregation_default: Optional[Callable] = None,
        aggregation_options: Union[Callable, Sequence[Callable]] = AGG.sum,
        aggregation_replace_rate: float = 0.1,
        activation_default: Optional[Callable] = None,
        activation_options: Union[Callable, Sequence[Callable]] = ACT.sigmoid,
        activation_replace_rate: float = 0.1,
    ):
        super().__init__()

        if isinstance(aggregation_options, Callable):
            aggregation_options = [aggregation_options]
        if isinstance(activation_options, Callable):
            activation_options = [activation_options]

        if aggregation_default is None:
            aggregation_default = aggregation_options[0]
        if activation_default is None:
            activation_default = activation_options[0]

        self.bias_init_mean = bias_init_mean
        self.bias_init_std = bias_init_std
        self.bias_mutate_power = bias_mutate_power
        self.bias_mutate_rate = bias_mutate_rate
        self.bias_replace_rate = bias_replace_rate
        self.bias_lower_bound = bias_lower_bound
        self.bias_upper_bound = bias_upper_bound

        self.response_init_mean = response_init_mean
        self.response_init_std = response_init_std
        self.response_mutate_power = response_mutate_power
        self.response_mutate_rate = response_mutate_rate
        self.response_replace_rate = response_replace_rate
        self.reponse_lower_bound = response_lower_bound
        self.response_upper_bound = response_upper_bound

        self.aggregation_default = aggregation_options.index(aggregation_default)
        self.aggregation_options = aggregation_options
        self.aggregation_indices = np.arange(len(aggregation_options))
        self.aggregation_replace_rate = aggregation_replace_rate

        self.activation_default = activation_options.index(activation_default)
        self.activation_options = activation_options
        self.activation_indices = np.arange(len(activation_options))
        self.activation_replace_rate = activation_replace_rate

    def new_identity_attrs(self, state):
        bias = 0
        res = 1
        agg = self.aggregation_default
        act = self.activation_default

        return jnp.array([bias, res, agg, act])  # activation=-1 means ACT.identity

    def new_random_attrs(self, state, randkey):
        k1, k2, k3, k4 = jax.random.split(randkey, num=4)
        bias = jax.random.normal(k1, ()) * self.bias_init_std + self.bias_init_mean
        bias = jnp.clip(bias, self.bias_lower_bound, self.bias_upper_bound)
        res = (
            jax.random.normal(k2, ()) * self.response_init_std + self.response_init_mean
        )
        res = jnp.clip(res, self.reponse_lower_bound, self.response_upper_bound)
        agg = jax.random.choice(k3, self.aggregation_indices)
        act = jax.random.choice(k4, self.activation_indices)

        return jnp.array([bias, res, agg, act])

    def mutate(self, state, randkey, attrs):
        k1, k2, k3, k4 = jax.random.split(randkey, num=4)
        bias, res, agg, act = attrs
        bias = mutate_float(
            k1,
            bias,
            self.bias_init_mean,
            self.bias_init_std,
            self.bias_mutate_power,
            self.bias_mutate_rate,
            self.bias_replace_rate,
        )
        bias = jnp.clip(bias, self.bias_lower_bound, self.bias_upper_bound)
        res = mutate_float(
            k2,
            res,
            self.response_init_mean,
            self.response_init_std,
            self.response_mutate_power,
            self.response_mutate_rate,
            self.response_replace_rate,
        )
        res = jnp.clip(res, self.reponse_lower_bound, self.response_upper_bound)
        agg = mutate_int(
            k4, agg, self.aggregation_indices, self.aggregation_replace_rate
        )

        act = mutate_int(k3, act, self.activation_indices, self.activation_replace_rate)

        return jnp.array([bias, res, agg, act])

    def distance(self, state, attrs1, attrs2):
        bias1, res1, agg1, act1 = attrs1
        bias2, res2, agg2, act2 = attrs2
        return (
            jnp.abs(bias1 - bias2)  # bias
            + jnp.abs(res1 - res2)  # response
            + (agg1 != agg2)  # aggregation
            + (act1 != act2)  # activation
        )

    def forward(self, state, attrs, inputs, is_output_node=False):
        bias, res, agg, act = attrs

        z = apply_aggregation(agg, inputs, self.aggregation_options)
        z = bias + res * z

        # the last output node should not be activated
        z = jax.lax.cond(
            is_output_node,
            lambda: z,
            lambda: apply_activation(act, z, self.activation_options),
        )

        return z

    def repr(self, state, node, precision=2, idx_width=3, func_width=8):
        idx, bias, res, agg, act = node

        idx = int(idx)
        bias = round(float(bias), precision)
        res = round(float(res), precision)
        agg = int(agg)
        act = int(act)

        if act == -1:
            act_func = ACT.identity
        else:
            act_func = self.activation_options[act]
        return "{}(idx={:<{idx_width}}, bias={:<{float_width}}, response={:<{float_width}}, aggregation={:<{func_width}}, activation={:<{func_width}})".format(
            self.__class__.__name__,
            idx,
            bias,
            res,
            get_func_name(self.aggregation_options[agg]),
            get_func_name(act_func),
            idx_width=idx_width,
            float_width=precision + 3,
            func_width=func_width,
        )

    def to_dict(self, state, node):
        idx, bias, res, agg, act = node

        idx = int(idx)
        bias = jnp.float32(bias)
        res = jnp.float32(res)
        agg = int(agg)
        act = int(act)

        if act == -1:
            act_func = ACT.identity
        else:
            act_func = self.activation_options[act]
        return {
            "idx": idx,
            "bias": bias,
            "res": res,
            "agg": get_func_name(self.aggregation_options[agg]),
            "act": get_func_name(act_func),
        }

    def sympy_func(self, state, node_dict, inputs, is_output_node=False):
        nd = node_dict
        bias = sp.symbols(f"n_{nd['idx']}_b")
        res = sp.symbols(f"n_{nd['idx']}_r")

        z = AGG.obtain_sympy(nd["agg"])(inputs)
        z = bias + res * z

        if is_output_node:
            pass
        else:
            z = ACT.obtain_sympy(nd["act"])(z)

        return z, {bias: nd["bias"], res: nd["res"]}

=== ./src/tensorneat/genome/gene/node/origin.py ===
import jax, jax.numpy as jnp
from .default import DefaultNode


class OriginNode(DefaultNode):
    """
    Implementation of nodes in origin NEAT Paper.
    Details at https://github.com/EMI-Group/tensorneat/issues/11.
    """

    def __init__(
        self,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)

    def crossover(self, state, randkey, attrs1, attrs2):
        # random pick one of attrs, without attrs exchange
        return jnp.where(
            # origin code, generate multiple random numbers, without attrs exchange
            # jax.random.normal(randkey, attrs1.shape) > 0,
            jax.random.normal(randkey)
            > 0,  # generate one random number, without attrs exchange
            attrs1,
            attrs2,
        )

=== ./src/tensorneat/genome/gene/node/base.py ===
import jax, jax.numpy as jnp
from .. import BaseGene


class BaseNode(BaseGene):
    "Base class for node genes."
    fixed_attrs = ["index"]

    def __init__(self):
        super().__init__()

    def forward(self, state, attrs, inputs, is_output_node=False):
        raise NotImplementedError

    def repr(self, state, node, precision=2, idx_width=3, func_width=8):
        idx = node[0]

        idx = int(idx)
        return "{}(idx={:<{idx_width}})".format(
            self.__class__.__name__, idx, idx_width=idx_width
        )

    def to_dict(self, state, node):
        idx = node[0]
        return {
            "idx": int(idx),
        }

    def sympy_func(self, state, node_dict, inputs, is_output_node=False):
        raise NotImplementedError

=== ./src/tensorneat/genome/gene/base.py ===
import jax, jax.numpy as jnp
from ...common import State, StatefulBaseClass, hash_array


class BaseGene(StatefulBaseClass):
    "Base class for node genes or connection genes."
    fixed_attrs = []
    custom_attrs = []

    def __init__(self):
        pass

    def new_identity_attrs(self, state):
        # the attrs which do identity transformation, used in mutate add node
        raise NotImplementedError

    def new_random_attrs(self, state, randkey):
        # random attributes of the gene. used in initialization.
        raise NotImplementedError

    def mutate(self, state, randkey, attrs):
        raise NotImplementedError

    def crossover(self, state, randkey, attrs1, attrs2):
        return jnp.where(
            jax.random.normal(randkey, attrs1.shape) > 0,
            attrs1,
            attrs2,
        )

    def distance(self, state, attrs1, attrs2):
        raise NotImplementedError

    def forward(self, state, attrs, inputs):
        raise NotImplementedError

    @property
    def length(self):
        return len(self.fixed_attrs) + len(self.custom_attrs)

    def repr(self, state, gene, precision=2):
        raise NotImplementedError

    def hash(self, gene):
        return hash_array(gene)

=== ./src/tensorneat/genome/gene/conn/__init__.py ===
from .base import BaseConn
from .default import DefaultConn
from .origin import OriginConn
=== ./src/tensorneat/genome/gene/conn/default.py ===
import jax.numpy as jnp
import jax.random
import sympy as sp
from ....common import mutate_float
from .base import BaseConn


class DefaultConn(BaseConn):
    "Default connection gene, with the same behavior as in NEAT-python."

    custom_attrs = ["weight"]

    def __init__(
        self,
        weight_init_mean: float = 0.0,
        weight_init_std: float = 1.0,
        weight_mutate_power: float = 0.15,
        weight_mutate_rate: float = 0.2,
        weight_replace_rate: float = 0.015,
        weight_lower_bound: float = -5.0,
        weight_upper_bound: float = 5.0,
    ):
        super().__init__()
        self.weight_init_mean = weight_init_mean
        self.weight_init_std = weight_init_std
        self.weight_mutate_power = weight_mutate_power
        self.weight_mutate_rate = weight_mutate_rate
        self.weight_replace_rate = weight_replace_rate
        self.weight_lower_bound = weight_lower_bound
        self.weight_upper_bound = weight_upper_bound

    def new_zero_attrs(self, state):
        return jnp.array([0.0])  # weight = 0

    def new_identity_attrs(self, state):
        return jnp.array([1.0])  # weight = 1

    def new_random_attrs(self, state, randkey):
        weight = (
            jax.random.normal(randkey, ()) * self.weight_init_std
            + self.weight_init_mean
        )
        weight = jnp.clip(weight, self.weight_lower_bound, self.weight_upper_bound)
        return jnp.array([weight])

    def mutate(self, state, randkey, attrs):
        weight = attrs[0]
        weight = mutate_float(
            randkey,
            weight,
            self.weight_init_mean,
            self.weight_init_std,
            self.weight_mutate_power,
            self.weight_mutate_rate,
            self.weight_replace_rate,
        )
        weight = jnp.clip(weight, self.weight_lower_bound, self.weight_upper_bound)
        return jnp.array([weight])

    def distance(self, state, attrs1, attrs2):
        weight1 = attrs1[0]
        weight2 = attrs2[0]
        return jnp.abs(weight1 - weight2)

    def forward(self, state, attrs, inputs):
        weight = attrs[0]
        return inputs * weight

    def repr(self, state, conn, precision=2, idx_width=3, func_width=8):
        in_idx, out_idx, weight = conn

        in_idx = int(in_idx)
        out_idx = int(out_idx)
        weight = round(float(weight), precision)

        return "{}(in: {:<{idx_width}}, out: {:<{idx_width}}, weight: {:<{float_width}})".format(
            self.__class__.__name__,
            in_idx,
            out_idx,
            weight,
            idx_width=idx_width,
            float_width=precision + 3,
        )

    def to_dict(self, state, conn):
        return {
            "in": int(conn[0]),
            "out": int(conn[1]),
            "weight": jnp.float32(conn[2]),
        }

    def sympy_func(self, state, conn_dict, inputs, precision=None):
        weight = sp.symbols(f"c_{conn_dict['in']}_{conn_dict['out']}_w")

        return inputs * weight, {weight: conn_dict["weight"]}

=== ./src/tensorneat/genome/gene/conn/origin.py ===
import jax, jax.numpy as jnp
from .default import DefaultConn


class OriginConn(DefaultConn):
    """
    Implementation of connections in origin NEAT Paper.
    Details at https://github.com/EMI-Group/tensorneat/issues/11.
    """

    # add historical_marker into fixed_attrs
    fixed_attrs = ["input_index", "output_index", "historical_marker"]
    custom_attrs = ["weight"]

    def __init__(
        self,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)

    def crossover(self, state, randkey, attrs1, attrs2):
        # random pick one of attrs, without attrs exchange
        return jnp.where(
            # origin code, generate multiple random numbers, without attrs exchange
            # jax.random.normal(randkey, attrs1.shape) > 0,
            jax.random.normal(randkey)
            > 0,  # generate one random number, without attrs exchange
            attrs1,
            attrs2,
        )

    def get_historical_marker(self, state, gene_array):
        return gene_array[2]
    
    def repr(self, state, conn, precision=2, idx_width=3, func_width=8):
        in_idx, out_idx, historical_marker, weight = conn

        in_idx = int(in_idx)
        out_idx = int(out_idx)
        historical_marker = int(historical_marker)
        weight = round(float(weight), precision)

        return "{}(in: {:<{idx_width}}, out: {:<{idx_width}}, historical_marker: {:<{idx_width}}, weight: {:<{float_width}})".format(
            self.__class__.__name__,
            in_idx,
            out_idx,
            historical_marker,
            weight,
            idx_width=idx_width,
            float_width=precision + 3,
        )

    def to_dict(self, state, conn):
        return {
            "in": int(conn[0]),
            "out": int(conn[1]),
            "historical_marker": int(conn[2]),
            "weight": jnp.float32(conn[3]),
        }
=== ./src/tensorneat/genome/gene/conn/base.py ===
from ..base import BaseGene


class BaseConn(BaseGene):
    "Base class for connection genes."
    fixed_attrs = ["input_index", "output_index"]

    def __init__(self):
        super().__init__()

    def new_zero_attrs(self, state):
        # the attrs which make the least influence on the network, used in mutate add conn
        raise NotImplementedError

    def forward(self, state, attrs, inputs):
        raise NotImplementedError

    def repr(self, state, conn, precision=2, idx_width=3, func_width=8):
        in_idx, out_idx = conn[:2]
        in_idx = int(in_idx)
        out_idx = int(out_idx)

        return "{}(in: {:<{idx_width}}, out: {:<{idx_width}})".format(
            self.__class__.__name__, in_idx, out_idx, idx_width=idx_width
        )

    def to_dict(self, state, conn):
        in_idx, out_idx = conn[:2]
        return {
            "in": int(in_idx),
            "out": int(out_idx),
        }

    def sympy_func(self, state, conn_dict, inputs):
        raise NotImplementedError

=== ./src/tensorneat/genome/operations/mutation/__init__.py ===
from .base import BaseMutation
from .default import DefaultMutation

=== ./src/tensorneat/genome/operations/mutation/default.py ===
import jax
from jax import vmap, numpy as jnp
from . import BaseMutation
from ....common import (
    fetch_first,
    fetch_random,
    I_INF,
    check_cycles,
)
from ...utils import (
    unflatten_conns,
    add_node,
    add_conn,
    delete_node_by_pos,
    delete_conn_by_pos,
    extract_gene_attrs,
    set_gene_attrs,
)


class DefaultMutation(BaseMutation):
    def __init__(
        self,
        conn_add: float = 0.2,
        conn_delete: float = 0.2,
        node_add: float = 0.1,
        node_delete: float = 0.1,
    ):
        self.conn_add = conn_add
        self.conn_delete = conn_delete
        self.node_add = node_add
        self.node_delete = node_delete

    def __call__(
        self, state, genome, randkey, nodes, conns, new_node_key, new_conn_key
    ):
        assert (
            new_node_key.shape == ()
        )  # scalar, as there is max one new node in each mutation
        assert new_conn_key.shape == (
            3,
        )  # there are max 3 new connections (mutate add node + mutate add conn)

        k1, k2 = jax.random.split(randkey)

        nodes, conns = self.mutate_structure(
            state, genome, k1, nodes, conns, new_node_key, new_conn_key
        )
        nodes, conns = self.mutate_values(state, genome, k2, nodes, conns)

        return nodes, conns

    def mutate_structure(
        self, state, genome, randkey, nodes, conns, new_node_key, new_conn_key
    ):
        def mutate_add_node(key_, nodes_, conns_):
            """
            add a node while do not influence the output of the network
            """

            remain_node_space = jnp.isnan(nodes_[:, 0]).sum()
            remain_conn_space = jnp.isnan(conns_[:, 0]).sum()
            i_key, o_key, idx = self.choose_connection_key(
                key_, conns_
            )  # choose a connection

            def successful_add_node():
                # remove the original connection and record its attrs
                original_attrs = extract_gene_attrs(genome.conn_gene, conns_[idx])
                new_conns = delete_conn_by_pos(conns_, idx)

                # add a new node with identity attrs
                new_nodes = add_node(
                    nodes_,
                    jnp.array([new_node_key]),
                    genome.node_gene.new_identity_attrs(state),
                )

                # whether to use historical marker in connection
                if "historical_marker" in genome.conn_gene.fixed_attrs:
                    fix_attrs1 = jnp.array([i_key, new_node_key, new_conn_key[0]])
                    fix_attrs2 = jnp.array([new_node_key, o_key, new_conn_key[1]])
                else:
                    fix_attrs1 = jnp.array([i_key, new_node_key])
                    fix_attrs2 = jnp.array([new_node_key, o_key])

                # add two new connections
                # first is with identity attrs
                new_conns = add_conn(
                    new_conns,
                    fix_attrs1,
                    genome.conn_gene.new_identity_attrs(state),
                )
                # second is with the origin attrs
                new_conns = add_conn(
                    new_conns,
                    fix_attrs2,
                    original_attrs,
                )

                return new_nodes, new_conns

            return jax.lax.cond(
                (idx == I_INF) | (remain_node_space < 1) | (remain_conn_space < 2),
                lambda: (nodes_, conns_),  # do nothing
                successful_add_node,
            )

        def mutate_delete_node(key_, nodes_, conns_):
            """
            delete a node
            """
            # randomly choose a node
            key, idx = self.choose_node_key(
                key_,
                nodes_,
                genome.input_idx,
                genome.output_idx,
                allow_input_keys=False,
                allow_output_keys=False,
            )

            def successful_delete_node():
                # delete the node
                new_nodes = delete_node_by_pos(nodes_, idx)

                # delete all connections
                new_conns = jnp.where(
                    ((conns_[:, 0] == key) | (conns_[:, 1] == key))[:, None],
                    jnp.nan,
                    conns_,
                )

                return new_nodes, new_conns

            return jax.lax.cond(
                idx == I_INF,  # no available node to delete
                lambda: (nodes_, conns_),  # do nothing
                successful_delete_node,
            )

        def mutate_add_conn(key_, nodes_, conns_):
            """
            add a connection while do not influence the output of the network
            """

            remain_conn_space = jnp.isnan(conns_[:, 0]).sum()

            # randomly choose two nodes
            k1_, k2_ = jax.random.split(key_, num=2)

            # input node of the connection can be any node
            i_key, from_idx = self.choose_node_key(
                k1_,
                nodes_,
                genome.input_idx,
                genome.output_idx,
                allow_input_keys=True,
                allow_output_keys=True,
            )

            # output node of the connection can be any node except input node
            o_key, to_idx = self.choose_node_key(
                k2_,
                nodes_,
                genome.input_idx,
                genome.output_idx,
                allow_input_keys=False,
                allow_output_keys=True,
            )

            conn_pos = fetch_first((conns_[:, 0] == i_key) & (conns_[:, 1] == o_key))
            is_already_exist = conn_pos != I_INF

            def nothing():
                return nodes_, conns_

            def successful():
                # add a connection with zero attrs
                if "historical_marker" in genome.conn_gene.fixed_attrs:
                    new_fix_attrs = jnp.array([i_key, o_key, new_conn_key[2]])
                else:
                    new_fix_attrs = jnp.array([i_key, o_key])
                return nodes_, add_conn(
                    conns_, new_fix_attrs, genome.conn_gene.new_zero_attrs(state)
                )

            if genome.network_type == "feedforward":
                u_conns = unflatten_conns(nodes_, conns_)
                conns_exist = u_conns != I_INF
                is_cycle = check_cycles(nodes_, conns_exist, from_idx, to_idx)

                return jax.lax.cond(
                    is_already_exist | is_cycle | (remain_conn_space < 1),
                    nothing,
                    successful,
                )

            elif genome.network_type == "recurrent":
                return jax.lax.cond(
                    is_already_exist | (remain_conn_space < 1),
                    nothing,
                    successful,
                )

            else:
                raise ValueError(f"Invalid network type: {genome.network_type}")

        def mutate_delete_conn(key_, nodes_, conns_):
            # randomly choose a connection
            i_key, o_key, idx = self.choose_connection_key(key_, conns_)

            return jax.lax.cond(
                idx == I_INF,
                lambda: (nodes_, conns_),  # nothing
                lambda: (nodes_, delete_conn_by_pos(conns_, idx)),  # success
            )

        k1, k2, k3, k4 = jax.random.split(randkey, num=4)
        r1, r2, r3, r4 = jax.random.uniform(k1, shape=(4,))

        def nothing(_, nodes_, conns_):
            return nodes_, conns_

        if self.node_add > 0:
            nodes, conns = jax.lax.cond(
                r1 < self.node_add, mutate_add_node, nothing, k1, nodes, conns
            )

        if self.node_delete > 0:
            nodes, conns = jax.lax.cond(
                r2 < self.node_delete, mutate_delete_node, nothing, k2, nodes, conns
            )

        if self.conn_add > 0:
            nodes, conns = jax.lax.cond(
                r3 < self.conn_add, mutate_add_conn, nothing, k3, nodes, conns
            )

        if self.conn_delete > 0:
            nodes, conns = jax.lax.cond(
                r4 < self.conn_delete, mutate_delete_conn, nothing, k4, nodes, conns
            )

        return nodes, conns

    def mutate_values(self, state, genome, randkey, nodes, conns):
        k1, k2 = jax.random.split(randkey)
        nodes_randkeys = jax.random.split(k1, num=genome.max_nodes)
        conns_randkeys = jax.random.split(k2, num=genome.max_conns)

        node_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(
            genome.node_gene, nodes
        )
        new_node_attrs = vmap(genome.node_gene.mutate, in_axes=(None, 0, 0))(
            state, nodes_randkeys, node_attrs
        )
        new_nodes = vmap(set_gene_attrs, in_axes=(None, 0, 0))(
            genome.node_gene, nodes, new_node_attrs
        )

        conn_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(
            genome.conn_gene, conns
        )
        new_conn_attrs = vmap(genome.conn_gene.mutate, in_axes=(None, 0, 0))(
            state, conns_randkeys, conn_attrs
        )
        new_conns = vmap(set_gene_attrs, in_axes=(None, 0, 0))(
            genome.conn_gene, conns, new_conn_attrs
        )

        # nan nodes not changed
        new_nodes = jnp.where(jnp.isnan(nodes), jnp.nan, new_nodes)
        new_conns = jnp.where(jnp.isnan(conns), jnp.nan, new_conns)

        return new_nodes, new_conns

    def choose_node_key(
        self,
        key,
        nodes,
        input_idx,
        output_idx,
        allow_input_keys: bool = False,
        allow_output_keys: bool = False,
    ):
        """
        Randomly choose a node key from the given nodes. It guarantees that the chosen node not be the input or output node.
        :param key:
        :param nodes:
        :param input_idx:
        :param output_idx:
        :param allow_input_keys:
        :param allow_output_keys:
        :return: return its key and position(idx)
        """

        node_keys = nodes[:, 0]
        mask = ~jnp.isnan(node_keys)

        if not allow_input_keys:
            mask = jnp.logical_and(mask, ~jnp.isin(node_keys, input_idx))

        if not allow_output_keys:
            mask = jnp.logical_and(mask, ~jnp.isin(node_keys, output_idx))

        idx = fetch_random(key, mask)
        key = jnp.where(idx != I_INF, nodes[idx, 0], jnp.nan)
        return key, idx

    def choose_connection_key(self, key, conns):
        """
        Randomly choose a connection key from the given connections.
        :return: i_key, o_key, idx
        """

        idx = fetch_random(key, ~jnp.isnan(conns[:, 0]))
        i_key = jnp.where(idx != I_INF, conns[idx, 0], jnp.nan)
        o_key = jnp.where(idx != I_INF, conns[idx, 1], jnp.nan)

        return i_key, o_key, idx

=== ./src/tensorneat/genome/operations/mutation/base.py ===
from ....common import StatefulBaseClass, State


class BaseMutation(StatefulBaseClass):

    def __call__(
        self, state, genome, randkey, nodes, conns, new_node_key, new_conn_key
    ):
        raise NotImplementedError

=== ./src/tensorneat/genome/operations/distance/__init__.py ===
from .base import BaseDistance
from .default import DefaultDistance

=== ./src/tensorneat/genome/operations/distance/default.py ===
from jax import vmap, numpy as jnp

from .base import BaseDistance
from ...gene import BaseGene
from ...utils import extract_gene_attrs


class DefaultDistance(BaseDistance):
    def __init__(
        self,
        compatibility_disjoint: float = 1.0,
        compatibility_weight: float = 0.4,
    ):
        self.compatibility_disjoint = compatibility_disjoint
        self.compatibility_weight = compatibility_weight

    def __call__(self, state, genome, nodes1, conns1, nodes2, conns2):
        """
        The distance between two genomes
        """
        node_distance = self.gene_distance(state, genome.node_gene, nodes1, nodes2)
        conn_distance = self.gene_distance(state, genome.conn_gene, conns1, conns2)
        return node_distance + conn_distance


    def gene_distance(self, state, gene: BaseGene, genes1, genes2):
        """
        The distance between to genes
        genes1: 2-D jax array with shape
        genes2: 2-D jax array with shape
        gene1.shape == gene2.shape
        """
        cnt1 = jnp.sum(~jnp.isnan(genes1[:, 0]))
        cnt2 = jnp.sum(~jnp.isnan(genes2[:, 0]))
        max_cnt = jnp.maximum(cnt1, cnt2)

        # align homologous nodes
        # this process is similar to np.intersect1d in higher dimension
        total_genes = jnp.concatenate((genes1, genes2), axis=0)
        identifiers = total_genes[:, : len(gene.fixed_attrs)]
        sorted_identifiers = jnp.lexsort(identifiers.T[::-1])
        total_genes = total_genes[sorted_identifiers]
        total_genes = jnp.concatenate(
            [total_genes, jnp.full((1, total_genes.shape[1]), jnp.nan)], axis=0
        )  # add a nan row to the end
        fr, sr = total_genes[:-1], total_genes[1:]  # first row, second row

        # intersect part of two genes
        intersect_mask = jnp.all(
            fr[:, : len(gene.fixed_attrs)] == sr[:, : len(gene.fixed_attrs)], axis=1
        ) & ~jnp.isnan(fr[:, 0])

        non_homologous_cnt = cnt1 + cnt2 - 2 * jnp.sum(intersect_mask)

        fr_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(gene, fr)
        sr_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(gene, sr)

        # homologous gene distance
        hgd = vmap(gene.distance, in_axes=(None, 0, 0))(state, fr_attrs, sr_attrs)
        hgd = jnp.where(jnp.isnan(hgd), 0, hgd)
        homologous_distance = jnp.sum(hgd * intersect_mask)

        val = (
            non_homologous_cnt * self.compatibility_disjoint
            + homologous_distance * self.compatibility_weight
        )

        val = jnp.where(max_cnt == 0, 0, val / max_cnt)  # normalize

        return val

=== ./src/tensorneat/genome/operations/distance/base.py ===
from ....common import StatefulBaseClass, State


class BaseDistance(StatefulBaseClass):

    def __call__(self, state, genome, nodes1, nodes2, conns1, conns2):
        """
        The distance between two genomes
        """
        raise NotImplementedError

=== ./src/tensorneat/genome/operations/__init__.py ===
from .crossover import BaseCrossover, DefaultCrossover
from .mutation import BaseMutation, DefaultMutation
from .distance import BaseDistance, DefaultDistance

=== ./src/tensorneat/genome/operations/crossover/__init__.py ===
from .base import BaseCrossover
from .default import DefaultCrossover

=== ./src/tensorneat/genome/operations/crossover/default.py ===
import jax
from jax import vmap, numpy as jnp

from .base import BaseCrossover
from ...utils import extract_gene_attrs, set_gene_attrs

from ....common import fetch_first, I_INF
from ....genome.gene import BaseGene


class DefaultCrossover(BaseCrossover):
    def __call__(self, state, genome, randkey, nodes1, conns1, nodes2, conns2):
        """
        use genome1 and genome2 to generate a new genome
        notice that genome1 should have higher fitness than genome2 (genome1 is winner!)
        """
        randkey1, randkey2 = jax.random.split(randkey, 2)
        node_randkeys = jax.random.split(randkey1, genome.max_nodes)
        conn_randkeys = jax.random.split(randkey2, genome.max_conns)
        batch_create_new_gene = jax.vmap(
            create_new_gene, in_axes=(None, 0, None, 0, 0, None, None)
        )

        # crossover nodes
        node_keys1, node_keys2 = (
            nodes1[:, 0 : len(genome.node_gene.fixed_attrs)],
            nodes2[:, 0 : len(genome.node_gene.fixed_attrs)],
        )
        node_attrs1 = vmap(extract_gene_attrs, in_axes=(None, 0))(
            genome.node_gene, nodes1
        )
        node_attrs2 = vmap(extract_gene_attrs, in_axes=(None, 0))(
            genome.node_gene, nodes2
        )

        new_node_attrs = batch_create_new_gene(
            state,
            node_randkeys,
            genome.node_gene,
            node_keys1,
            node_attrs1,
            node_keys2,
            node_attrs2,
        )
        new_nodes = vmap(set_gene_attrs, in_axes=(None, 0, 0))(
            genome.node_gene, nodes1, new_node_attrs
        )

        # crossover connections
        # all fixed_attrs together will use to identify a connection
        # if using historical marker, use it
        # related to issue: https://github.com/EMI-Group/tensorneat/issues/11
        conn_keys1, conn_keys2 = (
            conns1[:, 0 : len(genome.conn_gene.fixed_attrs)],
            conns2[:, 0 : len(genome.conn_gene.fixed_attrs)],
        )
        conn_attrs1 = vmap(extract_gene_attrs, in_axes=(None, 0))(
            genome.conn_gene, conns1
        )
        conn_attrs2 = vmap(extract_gene_attrs, in_axes=(None, 0))(
            genome.conn_gene, conns2
        )

        new_conn_attrs = batch_create_new_gene(
            state,
            conn_randkeys,
            genome.conn_gene,
            conn_keys1,
            conn_attrs1,
            conn_keys2,
            conn_attrs2,
        )
        new_conns = vmap(set_gene_attrs, in_axes=(None, 0, 0))(
            genome.conn_gene, conns1, new_conn_attrs
        )

        return new_nodes, new_conns


def create_new_gene(
    state,
    randkey,
    gene: BaseGene,
    gene_key,
    gene_attrs,
    genes_keys,
    genes_attrs,
):
    # find homologous genes
    homologous_idx = fetch_first(jnp.all(gene_key == genes_keys, axis=1))

    def none():  # no homologous, use winner's gene
        return gene_attrs

    def crossover():  # when homologous gene is found, execute crossover
        return gene.crossover(state, randkey, gene_attrs, genes_attrs[homologous_idx])

    new_attrs = jax.lax.cond(
        homologous_idx == I_INF,  # homologous gene is not found or current gene is nan
        none,
        crossover,
    )

    return new_attrs

=== ./src/tensorneat/genome/operations/crossover/base.py ===
from ....common import StatefulBaseClass, State


class BaseCrossover(StatefulBaseClass):

    def __call__(self, state, genome, randkey, nodes1, nodes2, conns1, conns2):
        raise NotImplementedError

=== ./src/tensorneat/genome/default.py ===
import warnings

import jax
from jax import vmap, numpy as jnp
import numpy as np
import sympy as sp

from .base import BaseGenome
from .gene import DefaultNode, DefaultConn
from .operations import DefaultMutation, DefaultCrossover, DefaultDistance
from .utils import unflatten_conns, extract_gene_attrs, extract_gene_attrs

from ..common import (
    topological_sort,
    topological_sort_python,
    I_INF,
    attach_with_inf,
    ACT,
    AGG,
)


class DefaultGenome(BaseGenome):
    """Default genome class, with the same behavior as the NEAT-Python"""

    network_type = "feedforward"

    def __init__(
        self,
        num_inputs: int,
        num_outputs: int,
        max_nodes=50,
        max_conns=100,
        node_gene=DefaultNode(),
        conn_gene=DefaultConn(),
        mutation=DefaultMutation(),
        crossover=DefaultCrossover(),
        distance=DefaultDistance(),
        output_transform=None,
        input_transform=None,
        init_hidden_layers=(),
    ):

        super().__init__(
            num_inputs,
            num_outputs,
            max_nodes,
            max_conns,
            node_gene,
            conn_gene,
            mutation,
            crossover,
            distance,
            output_transform,
            input_transform,
            init_hidden_layers,
        )

    def transform(self, state, nodes, conns):
        u_conns = unflatten_conns(nodes, conns)
        conn_exist = u_conns != I_INF

        seqs = topological_sort(nodes, conn_exist)

        return seqs, nodes, conns, u_conns

    def forward(self, state, transformed, inputs):
        if self.input_transform is not None:
            inputs = self.input_transform(inputs)

        cal_seqs, nodes, conns, u_conns = transformed
        batch_size = inputs.shape[0] if len(inputs.shape) > 1 else 1

        # Create batched initial values
        ini_vals = jnp.full((batch_size, self.max_nodes), jnp.nan)

        # Handle both batched and unbatched inputs
        if len(inputs.shape) == 1:
            inputs = inputs[None, :]  # Add batch dimension if not present

        # Set input values for all batches
        ini_vals = ini_vals.at[:, self.input_idx].set(inputs)

        nodes_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(self.node_gene, nodes)
        conns_attrs = vmap(extract_gene_attrs, in_axes=(None, 0))(self.conn_gene, conns)

        def cond_fun(carry):
            values, idx = carry
            return (idx < self.max_nodes) & (cal_seqs[idx] != I_INF)

        def body_func(carry):
            values, idx = carry
            i = cal_seqs[idx]

            def input_node():
                return values

            def otherwise():
                # Calculate connections
                conn_indices = u_conns[:, i]
                hit_attrs = attach_with_inf(conns_attrs, conn_indices)

                # Handle batched operations
                ins = vmap(
                    lambda vals: vmap(self.conn_gene.forward, in_axes=(None, 0, 0))(
                        state, hit_attrs, vals
                    ),
                    in_axes=0,
                )(values)

                # Calculate nodes for each batch
                z = vmap(
                    lambda ins: self.node_gene.forward(
                        state,
                        nodes_attrs[i],
                        ins,
                        is_output_node=jnp.isin(nodes[i, 0], self.output_idx),
                    )
                )(ins)

                # Update values for all batches
                new_values = values.at[:, i].set(z)
                return new_values

            values = jax.lax.cond(jnp.isin(i, self.input_idx), input_node, otherwise)
            return values, idx + 1

        final_vals, _ = jax.lax.while_loop(cond_fun, body_func, (ini_vals, 0))

        # Get outputs and handle transformation
        outputs = final_vals[:, self.output_idx]
        if self.output_transform is None:
            return outputs
        else:
            return vmap(self.output_transform)(outputs)

    def network_dict(self, state, nodes, conns):
        network = super().network_dict(state, nodes, conns)
        topo_order, topo_layers = topological_sort_python(
            set(network["nodes"]), set(network["conns"])
        )
        network["topo_order"] = topo_order
        network["topo_layers"] = topo_layers
        return network

    def sympy_func(
        self,
        state,
        network,
        sympy_input_transform=None,
        sympy_output_transform=None,
        backend="jax",
    ):

        assert backend in ["jax", "numpy"], "backend should be 'jax' or 'numpy'"

        if sympy_input_transform is None and self.input_transform is not None:
            warnings.warn(
                "genome.input_transform is not None but sympy_input_transform is None!"
            )

        if sympy_input_transform is None:
            sympy_input_transform = lambda x: x

        if sympy_input_transform is not None:
            if not isinstance(sympy_input_transform, list):
                sympy_input_transform = [sympy_input_transform] * self.num_inputs

        if sympy_output_transform is None and self.output_transform is not None:
            warnings.warn(
                "genome.output_transform is not None but sympy_output_transform is None!"
            )

        input_idx = self.get_input_idx()
        output_idx = self.get_output_idx()
        order = network["topo_order"]

        hidden_idx = [
            i for i in network["nodes"] if i not in input_idx and i not in output_idx
        ]
        symbols = {}
        for i in network["nodes"]:
            if i in input_idx:
                symbols[-i - 1] = sp.Symbol(f"i{i - min(input_idx)}")  # origin_i
                symbols[i] = sp.Symbol(f"norm{i - min(input_idx)}")
            elif i in output_idx:
                symbols[i] = sp.Symbol(f"o{i - min(output_idx)}")
            else:  # hidden
                symbols[i] = sp.Symbol(f"h{i - min(hidden_idx)}")

        nodes_exprs = {}
        args_symbols = {}
        for i in order:

            if i in input_idx:
                nodes_exprs[symbols[-i - 1]] = symbols[
                    -i - 1
                ]  # origin equal to its symbol
                nodes_exprs[symbols[i]] = sympy_input_transform[i - min(input_idx)](
                    symbols[-i - 1]
                )  # normed i

            else:
                in_conns = [c for c in network["conns"] if c[1] == i]
                node_inputs = []
                for conn in in_conns:
                    val_represent = symbols[conn[0]]
                    # a_s -> args_symbols
                    val, a_s = self.conn_gene.sympy_func(
                        state,
                        network["conns"][conn],
                        val_represent,
                    )
                    args_symbols.update(a_s)
                    node_inputs.append(val)
                nodes_exprs[symbols[i]], a_s = self.node_gene.sympy_func(
                    state,
                    network["nodes"][i],
                    node_inputs,
                    is_output_node=(i in output_idx),
                )
                args_symbols.update(a_s)

                if i in output_idx and sympy_output_transform is not None:
                    nodes_exprs[symbols[i]] = sympy_output_transform(
                        nodes_exprs[symbols[i]]
                    )

        input_symbols = [symbols[-i - 1] for i in input_idx]
        reduced_exprs = nodes_exprs.copy()
        for i in order:
            reduced_exprs[symbols[i]] = reduced_exprs[symbols[i]].subs(reduced_exprs)

        output_exprs = [reduced_exprs[symbols[i]] for i in output_idx]

        lambdify_output_funcs = [
            sp.lambdify(
                input_symbols + list(args_symbols.keys()),
                exprs,
                modules=[backend, AGG.sympy_module(backend), ACT.sympy_module(backend)],
            )
            for exprs in output_exprs
        ]

        fixed_args_output_funcs = []
        for i in range(len(output_idx)):

            def f(inputs, i=i):
                return lambdify_output_funcs[i](*inputs, *args_symbols.values())

            fixed_args_output_funcs.append(f)

        forward_func = lambda inputs: jnp.array(
            [f(inputs) for f in fixed_args_output_funcs]
        )

        return (
            symbols,
            args_symbols,
            input_symbols,
            nodes_exprs,
            output_exprs,
            forward_func,
        )

    def visualize(
        self,
        network,
        rotate=0,
        reverse_node_order=False,
        size=(300, 300, 300),
        color=("yellow", "white", "blue"),
        with_labels=False,
        edgecolors="k",
        arrowstyle="->",
        arrowsize=3,
        edge_color=(0.3, 0.3, 0.3),
        save_path="network.svg",
        save_dpi=800,
        **kwargs,
    ):
        import networkx as nx
        from matplotlib import pyplot as plt

        conns_list = list(network["conns"])
        input_idx = self.get_input_idx()
        output_idx = self.get_output_idx()

        topo_order, topo_layers = network["topo_order"], network["topo_layers"]
        node2layer = {
            node: layer for layer, nodes in enumerate(topo_layers) for node in nodes
        }
        if reverse_node_order:
            topo_order = topo_order[::-1]

        G = nx.DiGraph()

        if not isinstance(size, tuple):
            size = (size, size, size)
        if not isinstance(color, tuple):
            color = (color, color, color)

        for node in topo_order:
            if node in input_idx:
                G.add_node(node, subset=node2layer[node], size=size[0], color=color[0])
            elif node in output_idx:
                G.add_node(node, subset=node2layer[node], size=size[2], color=color[2])
            else:
                G.add_node(node, subset=node2layer[node], size=size[1], color=color[1])

        for conn in conns_list:
            G.add_edge(conn[0], conn[1])
        pos = nx.multipartite_layout(G)

        def rotate_layout(pos, angle):
            angle_rad = np.deg2rad(angle)
            cos_angle, sin_angle = np.cos(angle_rad), np.sin(angle_rad)
            rotated_pos = {}
            for node, (x, y) in pos.items():
                rotated_pos[node] = (
                    cos_angle * x - sin_angle * y,
                    sin_angle * x + cos_angle * y,
                )
            return rotated_pos

        rotated_pos = rotate_layout(pos, rotate)

        node_sizes = [n["size"] for n in G.nodes.values()]
        node_colors = [n["color"] for n in G.nodes.values()]

        nx.draw(
            G,
            pos=rotated_pos,
            node_size=node_sizes,
            node_color=node_colors,
            with_labels=with_labels,
            edgecolors=edgecolors,
            arrowstyle=arrowstyle,
            arrowsize=arrowsize,
            edge_color=edge_color,
            **kwargs,
        )
        plt.savefig(save_path, dpi=save_dpi)
        plt.close()

=== ./src/tensorneat/genome/utils.py ===
import jax
from jax import vmap, numpy as jnp
import numpy as np

from .gene import BaseGene
from ..common import fetch_first, I_INF


def unflatten_conns(nodes, conns):
    """
    transform the (C, CL) connections to (N, N), which contains the idx of the connection in conns
    connection length, N means the number of nodes, C means the number of connections
    returns the unflatten connection indices with shape (N, N)
    """
    N = nodes.shape[0]  # max_nodes
    C = conns.shape[0]  # max_conns
    node_keys = nodes[:, 0]
    i_keys, o_keys = conns[:, 0], conns[:, 1]

    def key_to_indices(key, keys):
        return fetch_first(key == keys)

    i_idxs = vmap(key_to_indices, in_axes=(0, None))(i_keys, node_keys)
    o_idxs = vmap(key_to_indices, in_axes=(0, None))(o_keys, node_keys)

    # Is interesting that jax use clip when attach data in array
    # however, it will do nothing when setting values in an array
    # put the index of connections in the unflatten array
    unflatten = (
        jnp.full((N, N), I_INF, dtype=jnp.int32)
        .at[i_idxs, o_idxs]
        .set(jnp.arange(C, dtype=jnp.int32))
    )

    return unflatten


def valid_cnt(nodes_or_conns):
    return jnp.sum(~jnp.isnan(nodes_or_conns[:, 0]))


def extract_gene_attrs(gene: BaseGene, gene_array):
    """
    extract the custom attributes of the gene
    """
    return gene_array[len(gene.fixed_attrs) :]


def set_gene_attrs(gene: BaseGene, gene_array, attrs):
    """
    set the custom attributes of the gene
    """
    return gene_array.at[len(gene.fixed_attrs) :].set(attrs)


def add_node(nodes, fix_attrs, custom_attrs):
    """
    Add a new node to the genome.
    The new node will place at the first NaN row.
    """
    pos = fetch_first(jnp.isnan(nodes[:, 0]))
    return nodes.at[pos].set(jnp.concatenate((fix_attrs, custom_attrs)))


def delete_node_by_pos(nodes, pos):
    """
    Delete a node from the genome.
    Delete the node by its pos in nodes.
    """
    return nodes.at[pos].set(jnp.nan)


def add_conn(conns, fix_attrs, custom_attrs):
    """
    Add a new connection to the genome.
    The new connection will place at the first NaN row.
    """
    pos = fetch_first(jnp.isnan(conns[:, 0]))
    return conns.at[pos].set(jnp.concatenate((fix_attrs, custom_attrs)))


def delete_conn_by_pos(conns, pos):
    """
    Delete a connection from the genome.
    Delete the connection by its idx.
    """
    return conns.at[pos].set(jnp.nan)


def re_cound_idx(nodes, conns, input_idx, output_idx):
    """
    Make the key of hidden nodes continuous.
    Also update the index of connections.
    """
    nodes, conns = jax.device_get((nodes, conns))
    next_key = max(*input_idx, *output_idx) + 1
    old2new = {}
    for i, key in enumerate(nodes[:, 0]):
        if np.isnan(key):
            continue
        if np.in1d(key, input_idx + output_idx):
            continue
        old2new[int(key)] = next_key
        next_key += 1

    new_nodes = nodes.copy()
    for i, key in enumerate(nodes[:, 0]):
        if (not np.isnan(key)) and int(key) in old2new:
            new_nodes[i, 0] = old2new[int(key)]

    new_conns = conns.copy()
    for i, (i_key, o_key) in enumerate(conns[:, :2]):
        if (not np.isnan(i_key)) and int(i_key) in old2new:
            new_conns[i, 0] = old2new[int(i_key)]
        if (not np.isnan(o_key)) and int(o_key) in old2new:
            new_conns[i, 1] = old2new[int(o_key)]
    return new_nodes, new_conns

=== ./src/tensorneat/genome/base.py ===
from typing import Callable, Sequence

import numpy as np
import jax
from jax import vmap, numpy as jnp
from .gene import BaseNode, BaseConn
from .operations import BaseMutation, BaseCrossover, BaseDistance
from ..common import (
    State,
    StatefulBaseClass,
    hash_array,
)
from .utils import valid_cnt, re_cound_idx


class BaseGenome(StatefulBaseClass):
    network_type = None

    def __init__(
        self,
        num_inputs: int,
        num_outputs: int,
        max_nodes: int,
        max_conns: int,
        node_gene: BaseNode,
        conn_gene: BaseConn,
        mutation: BaseMutation,
        crossover: BaseCrossover,
        distance: BaseDistance,
        output_transform: Callable = None,
        input_transform: Callable = None,
        init_hidden_layers: Sequence[int] = (),
    ):

        # check transform functions
        if input_transform is not None:
            try:
                _ = input_transform(jnp.zeros(num_inputs))
            except Exception as e:
                raise ValueError(f"Input transform function failed: {e}")

        if output_transform is not None:
            try:
                _ = output_transform(jnp.zeros(num_outputs))
            except Exception as e:
                raise ValueError(f"Output transform function failed: {e}")

        # prepare for initialization
        all_layers = [num_inputs] + list(init_hidden_layers) + [num_outputs]
        layer_indices = []
        next_index = 0
        for layer in all_layers:
            layer_indices.append(list(range(next_index, next_index + layer)))
            next_index += layer

        all_init_nodes = []
        all_init_conns_in_idx = []
        all_init_conns_out_idx = []
        for i in range(len(layer_indices) - 1):
            in_layer = layer_indices[i]
            out_layer = layer_indices[i + 1]
            for in_idx in in_layer:
                for out_idx in out_layer:
                    all_init_conns_in_idx.append(in_idx)
                    all_init_conns_out_idx.append(out_idx)
            all_init_nodes.extend(in_layer)
        all_init_nodes.extend(layer_indices[-1])  # output layer

        if max_nodes < len(all_init_nodes):
            raise ValueError(
                f"max_nodes={max_nodes} must be greater than or equal to the number of initial nodes={len(all_init_nodes)}"
            )

        if max_conns < len(all_init_conns_in_idx):
            raise ValueError(
                f"max_conns={max_conns} must be greater than or equal to the number of initial connections={len(all_init_conns_in_idx)}"
            )

        self.num_inputs = num_inputs
        self.num_outputs = num_outputs
        self.max_nodes = max_nodes
        self.max_conns = max_conns
        self.node_gene = node_gene
        self.conn_gene = conn_gene
        self.mutation = mutation
        self.crossover = crossover
        self.distance = distance
        self.output_transform = output_transform
        self.input_transform = input_transform

        self.input_idx = np.array(layer_indices[0])
        self.output_idx = np.array(layer_indices[-1])
        self.all_init_nodes = np.array(all_init_nodes)
        self.all_init_conns = np.c_[all_init_conns_in_idx, all_init_conns_out_idx]

    def setup(self, state=State()):
        state = self.node_gene.setup(state)
        state = self.conn_gene.setup(state)
        state = self.mutation.setup(state)
        state = self.crossover.setup(state)
        state = self.distance.setup(state)
        return state

    def transform(self, state, nodes, conns):
        raise NotImplementedError

    def forward(self, state, transformed, inputs):
        raise NotImplementedError

    def sympy_func(self):
        raise NotImplementedError

    def visualize(self):
        raise NotImplementedError

    def execute_mutation(
        self, state, randkey, nodes, conns, new_node_key, new_conn_keys
    ):
        return self.mutation(
            state, self, randkey, nodes, conns, new_node_key, new_conn_keys
        )

    def execute_crossover(self, state, randkey, nodes1, conns1, nodes2, conns2):
        return self.crossover(state, self, randkey, nodes1, conns1, nodes2, conns2)

    def execute_distance(self, state, nodes1, conns1, nodes2, conns2):
        return self.distance(state, self, nodes1, conns1, nodes2, conns2)

    def initialize(self, state, randkey):
        k1, k2 = jax.random.split(randkey)  # k1 for nodes, k2 for conns

        all_nodes_cnt = len(self.all_init_nodes)
        all_conns_cnt = len(self.all_init_conns)

        # initialize nodes
        nodes = jnp.full((self.max_nodes, self.node_gene.length), jnp.nan)
        # create node indices
        node_indices = self.all_init_nodes
        # create node attrs
        rand_keys_n = jax.random.split(k1, num=all_nodes_cnt)
        node_attr_func = vmap(self.node_gene.new_random_attrs, in_axes=(None, 0))
        node_attrs = node_attr_func(state, rand_keys_n)

        nodes = nodes.at[:all_nodes_cnt, 0].set(node_indices)  # set node indices
        nodes = nodes.at[:all_nodes_cnt, 1:].set(node_attrs)  # set node attrs

        # initialize conns
        conns = jnp.full((self.max_conns, self.conn_gene.length), jnp.nan)
        # create input and output indices
        conn_indices = self.all_init_conns

        # create connection initial history markers
        conn_markers = jnp.arange(all_conns_cnt)

        # create conn attrs
        rand_keys_c = jax.random.split(k2, num=all_conns_cnt)
        conns_attrs = jax.vmap(
            self.conn_gene.new_random_attrs,
            in_axes=(
                None,
                0,
            ),
        )(state, rand_keys_c)

        # set conn indices
        conns = conns.at[:all_conns_cnt, :2].set(conn_indices)

        # set conn history markers if needed
        if "historical_marker" in self.conn_gene.fixed_attrs:
            conns = conns.at[:all_conns_cnt, 2].set(conn_markers)

        # set conn attrs
        conns = conns.at[:all_conns_cnt, len(self.conn_gene.fixed_attrs) :].set(
            conns_attrs
        )

        return nodes, conns

    def network_dict(self, state, nodes, conns, whether_re_cound_idx=True):
        if whether_re_cound_idx:
            nodes, conns = re_cound_idx(
                nodes, conns, self.get_input_idx(), self.get_output_idx()
            )
        return {
            "nodes": self._get_node_dict(state, nodes),
            "conns": self._get_conn_dict(state, conns),
        }

    def get_input_idx(self):
        return self.input_idx.tolist()

    def get_output_idx(self):
        return self.output_idx.tolist()

    def hash(self, nodes, conns):
        nodes_hashs = vmap(hash_array)(nodes)
        conns_hashs = vmap(hash_array)(conns)
        return hash_array(jnp.concatenate([nodes_hashs, conns_hashs]))

    def repr(self, state, nodes, conns, precision=2):
        nodes, conns = jax.device_get([nodes, conns])
        nodes_cnt, conns_cnt = valid_cnt(nodes), valid_cnt(conns)
        s = f"{self.__class__.__name__}(nodes={nodes_cnt}, conns={conns_cnt}):\n"
        s += f"\tNodes:\n"
        for node in nodes:
            if np.isnan(node[0]):
                break
            s += f"\t\t{self.node_gene.repr(state, node, precision=precision)}"
            node_idx = int(node[0])
            if np.isin(node_idx, self.input_idx):
                s += " (input)"
            elif np.isin(node_idx, self.output_idx):
                s += " (output)"
            s += "\n"

        s += f"\tConns:\n"
        for conn in conns:
            if np.isnan(conn[0]):
                break
            s += f"\t\t{self.conn_gene.repr(state, conn, precision=precision)}\n"
        return s

    def _get_conn_dict(self, state, conns):
        conns = jax.device_get(conns)
        conn_dict = {}
        for conn in conns:
            if np.isnan(conn[0]):
                continue
            cd = self.conn_gene.to_dict(state, conn)
            in_idx, out_idx = cd["in"], cd["out"]
            conn_dict[(in_idx, out_idx)] = cd
        return conn_dict

    def _get_node_dict(self, state, nodes):
        nodes = jax.device_get(nodes)
        node_dict = {}
        for node in nodes:
            if np.isnan(node[0]):
                continue
            nd = self.node_gene.to_dict(state, node)
            idx = nd["idx"]
            node_dict[idx] = nd
        return node_dict
